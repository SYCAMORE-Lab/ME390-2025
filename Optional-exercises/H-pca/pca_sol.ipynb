{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "**Author:** Sabri El Amrani\n",
    "\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align all tables to the left (useful for later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "sns.set()\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Background:__\n",
    "As a data scientist, you are tasked with analyzing a subset of leaf samples from an experiment conducted at INRA in Angers, France. The dataset includes reflectance measurements across wavelengths from 400 to 2450 nm for 18 leaf samples. Your objective is to apply dimensionality reduction and clustering techniques to uncover patterns and group the samples by species, which are unknown in this subset. Understanding these spectral profiles and identifying distinct species will provide insights into plant classification and the relationship between spectral data and species characteristics.*\n",
    "\n",
    "<img src=\"images/leaf.jpg\" style=\"width:700px\"/>\n",
    "\n",
    "[Source](https://www.flickr.com/photos/bob_81667/24688196150)\n",
    "\n",
    "Here is a link to the [dataset](https://ecosis.org/package/angers-leaf-optical-properties-database--2003-). This notebook and the next take inspiration from a case study found in the book [Machine Learning for Engineers](https://link.springer.com/book/10.1007/978-3-030-70388-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pandas, a data table is called a DataFrame (abbreviated to df)\n",
    "df = pd.read_csv('data/angers-leaf-optical-properties.csv')\n",
    "\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "# Show the first 5 rows of the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we only have 18 samples and we are doing unsupervised learning, we use all data for training\n",
    "X_train, y_train, X_test, y_test, feature_names, label_map = helpers.preprocess_data(df=df, label=\"English Name\", train_size=1.0, seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just revealed the 3 species from which our leaf samples are taken. In next week's notebook on clustering we will, however, forget about this for the sake of the exercise ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the reflectance spectra of our training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelength range from 400 to 2450\n",
    "wavelength = np.arange(400, 2451)\n",
    "\n",
    "# Define colors for each label\n",
    "colors = ['blue', 'green', 'red']\n",
    "label_colors = {label: colors[i] for i, label in enumerate(label_map.keys())}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot each row in X_train with consistent colors based on labels\n",
    "for i in range(X_train.shape[0]):\n",
    "    label = y_train[i]\n",
    "    plt.plot(wavelength, X_train[i], color=label_colors[label], label=label_map[label])\n",
    "\n",
    "# Adding legend (unique labels only)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "# Adding axis labels and title\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three species seem to have fairly similar behaviour, though some differences do appear. To faciliate next week's clustering task, let's try to reduce the dimensionality of our samples (currently 2051, which is a lot, especially for a dataset of only 18 samples!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get into the topic of the day: dimensionality reduction using principal component analysis, a.k.a. PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder of the algorithm:\n",
    "\n",
    "1. __Standardization:__  Standardize (a.k.a. normalize) the dataset to have a mean of zero and a variance of one.\n",
    "\n",
    "2. __Compute Covariance Matrix:__ Calculate the covariance matrix of the standardized data. The covariance matrix captures the variance and the relationship between different features in the dataset.\n",
    "\n",
    "3. __Compute & Sort Eigenvalues and Eigenvectors:__ Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues represent the variance explained by each principal component, and the eigenvectors represent the direction of the principal components. Don't forget to sort the eigenvalues in descending order and arrange the eigenvectors accordingly. The eigenvalues and their corresponding eigenvectors are sorted to prioritize the principal components that explain the most variance.\n",
    "\n",
    "4. __Transform Data:__ Transform the standardized data into the new PCA space by projecting it onto the eigenvectors. This results in a new feature matrix where the columns are the principal components.\n",
    "\n",
    "5. __Compute Explained Variance:__ Calculate the explained variance ratio for each principal component by dividing each eigenvalue by the total sum of eigenvalues. This step helps in understanding the proportion of variance each principal component explains.\n",
    "\n",
    "6. __Dimensionality Reduction:__ The algorithm outputs the transformed data (principal components), the explained variance ratios, and the eigenvectors (principal component directions). Using the explained variance, we can choose how many dimensions we keep to represent our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the various steps one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation for each feature of the training set\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Implement the normalize function\n",
    "def normalize(X: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
    "    \"\"\" Normalization of array using Z-score standardization\n",
    "     Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        mean: Mean of shape (D, )\n",
    "        std: Standard deviation of shape(D, )\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    X_normalized = (X - mean) / std\n",
    "    ### END CODE HERE ###\n",
    "    return X_normalized\n",
    "\n",
    "# Normalize features of the training, val and test set using the mean and std of the training set features\n",
    "X_train = normalize(X_train, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(X_standardized):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix of the standardized data.\n",
    "\n",
    "    Parameters:\n",
    "    X_standardized (np.ndarray): Standardized feature matrix.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Covariance matrix.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    return np.cov(X_standardized, rowvar=False)\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "cov_matrix = compute_covariance_matrix(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compute & Sort Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalues_and_eigenvectors(cov_matrix):\n",
    "    \"\"\"\n",
    "    Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cov_matrix (np.ndarray): Covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Sorted eigenvalues and corresponding eigenvectors.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    # Compute the eigenvalues and eigenvectors\n",
    "    # Hint: check np.linalg.eig\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    # Convert to real values if they have a negligible imaginary part\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Sort the eigenvalues and eigenvectors in descending order of the eigenvalues\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "    return sorted_eigenvalues, sorted_eigenvectors\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "eig_val, eig_vec = compute_eigenvalues_and_eigenvectors(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the data onto the computed eigenvectors.\n",
    "\n",
    "__Note:__ Though not relevant in this notebook, we can also project the validation and test sets into the PCA space, but using the eigenvectors computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X_standardized, eigenvectors):\n",
    "    \"\"\"\n",
    "    Transform the data into the new PCA space.\n",
    "\n",
    "    Parameters:\n",
    "    X_standardized (np.ndarray): Standardized feature matrix.\n",
    "    eigenvectors (np.ndarray): Eigenvectors of the covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Transformed feature matrix.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    return np.dot(X_standardized, eigenvectors)\n",
    "#   ## END CODE HERE ### \n",
    "\n",
    "X_pca_train = transform_data(X_train, eig_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Compute Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/formula.png\" style=\"width:500px\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/principal-component-analysis-ac90b73f68f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_explained_variance(eigenvalues):\n",
    "    \"\"\"\n",
    "    Compute the explained variance ratio for each principal component.\n",
    "\n",
    "    Parameters:\n",
    "    eigenvalues (np.ndarray): Eigenvalues of the covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Explained variance ratios.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    return eigenvalues / np.sum(eigenvalues)\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "explained_variance = compute_explained_variance(eig_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_explained_variance(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How many dimensions should we keep to explain at least 90 % of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ 3, value from which the cumulative explained variance exceeds 0.9 on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Dimensionaliy Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reconstruct the training data from the 3 first principal components only (i.e. retransform to the original space). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(X_pca, eigenvectors, n_components, mean, std):\n",
    "    \"\"\"\n",
    "    Reconstruct the data from the PCA-transformed space using a subset of principal components and denormalize it.\n",
    "\n",
    "    Parameters:\n",
    "    X_pca (np.ndarray): Transformed feature matrix with all principal components.\n",
    "    eigenvectors (np.ndarray): Eigenvectors of the covariance matrix.\n",
    "    n_components (int): Number of principal components to use for reconstruction.\n",
    "    mean (np.ndarray): Mean used for standardizing the original data.\n",
    "    std (np.ndarray): Standard deviation used for standardizing the original data.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Reconstructed and denormalized feature matrix in the original space.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    # Keep only the first n_components of X_pca\n",
    "    X_pca_reduced = X_pca[:, :n_components]\n",
    "    \n",
    "    # Select the first n_components eigenvectors\n",
    "    selected_eigenvectors = eigenvectors[:, :n_components]\n",
    "    \n",
    "    # Reconstruct the original data\n",
    "    X_reconstructed = np.dot(X_pca_reduced, selected_eigenvectors.T)\n",
    "    \n",
    "    # Denormalize the reconstructed data\n",
    "    X_reconstructed = X_reconstructed * std + mean\n",
    "    \n",
    "    return X_reconstructed\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "# Example usage\n",
    "n_components = 3  # Specify the number of principal components to use for reconstruction\n",
    "X_reconstructed_train = reconstruct_data(X_pca_train, eig_vec, n_components, mean, std,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot these reduced vectors. How do they compare with the original plots we made in Section 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelength range from 400 to 2450\n",
    "wavelength = np.arange(400, 2451)\n",
    "\n",
    "# Define colors for each label\n",
    "colors = ['blue', 'green', 'red']\n",
    "label_colors = {label: colors[i] for i, label in enumerate(label_map.keys())}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot each row in X_reconstructed_train with consistent colors based on labels\n",
    "for i in range(X_reconstructed_train.shape[0]):\n",
    "    label = y_train[i]\n",
    "    plt.plot(wavelength, X_reconstructed_train[i], color=label_colors[label], label=label_map[label])\n",
    "\n",
    "# Adding legend (unique labels only)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "# Adding axis labels and title\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good right?\n",
    "\n",
    "__Note:__ Remember that we reduced the dimensionality of our dataset from 2051 to 3, which is a massive reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save projected data for later processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of visualization and for later processing in the upcoming notebook on k-means, let's now keep our first two principal components and visualize them in the projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_pca_scatter(X_pca_train, y_train, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preliminary question for next notebook:__ Do the data points form clusters we can visualize in this reduced PCA space? If so, how many?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ To be determined in the notebook on k-means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pca_preprocessed_angers_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump((X_pca_train, y_train, label_map), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes this tutorial on PCA. Thank you for taking part! In the next notebook we will use k-means to determine how our projected data clusters, and whether these identified clusters match the underlying species."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
