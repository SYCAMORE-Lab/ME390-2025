{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a56257",
   "metadata": {},
   "source": [
    "# Intro to scikit-learn and decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca17b7c",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This notebook is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course and for the ME-390 Course at EPFL. Copyright (c) 2021 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** David Mizrahi\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f77ef0",
   "metadata": {},
   "source": [
    "This is the final exercise of this course. In this exercise, we'll introduce the scikit-learn package, and use it to train decision trees. We'll end with a small note on how to use scikit-learn for unsupervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6713a",
   "metadata": {},
   "source": [
    "## 1. Intro to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6b3ca",
   "metadata": {},
   "source": [
    "[scikit-learn](https://scikit-learn.org/stable/index.html) is a very popular Python package, built on top of NumPy, which provides efficient implementations of many popular machine learning algorithms.\n",
    "\n",
    "It can be used for:\n",
    "- Generating and loading popular datasets\n",
    "- Preprocessing (feature extraction and expansion, normalization)\n",
    "- Supervised learning (classification and regression)\n",
    "- Unsupervised learning (clustering and dimensionality reduction)\n",
    "- Model selection (grid search, train/test split, cross-validation)\n",
    "- Evaluation (with many metrics for all kinds of tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f5d3e",
   "metadata": {},
   "source": [
    "### 1.1. Data representation in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bdacd",
   "metadata": {},
   "source": [
    "In scikit-learn, data is represented in the same way it was in the previous exercises. That is:\n",
    "- The features are represented as a 2D features matrix (usually named `X`), most often contained in a NumPy array or Pandas DataFrame. \n",
    "- The label (or target) array is often called `y`, and is usually contained in a NumPy array or Pandas Series.\n",
    "- Note: For more information on Pandas package, you can optionally complete the 08-pandas exercises. Both the exercise questions and solutions are provided to you. \n",
    "\n",
    "In mathematical notation, this is:\n",
    "- features: $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times d}$, $\\forall \\ \\boldsymbol{x}^{i} \\in \\boldsymbol{X}: \\boldsymbol{x}^{i} \\in \\mathbb{R}^{d}$\n",
    "- label (or target): $\\boldsymbol{y} \\in \\mathbb{R}^{N}$  \n",
    "where $N$ is the number of examples in our dataset, and $d$ is the number of features per example  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05cf5",
   "metadata": {},
   "source": [
    "scikit-learn offers many utilities for splitting and preprocessing data. \n",
    "- For splitting data, there are functions such as [`model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) which splits arrays or matrices into random train and test subsets, or [`model_selection.KFold()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and similar functions which provides train/test indices for cross-validation. These functions are extremely handy, and are often used to split NumPy or Pandas arrays even when the training and models come from a library other than scikit-learn.\n",
    "- For preprocessing data, scikit-learn offers many utility functions which can standardize data (e.g. [`preprocessing.StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)), impute, discretize and perform feature expansion. For more information, refer to the [official preprocessing tutorial](https://scikit-learn.org/stable/modules/preprocessing.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e360ce1",
   "metadata": {},
   "source": [
    "### 1.2. Estimator API\n",
    "\n",
    "\n",
    "For **supervised learning**, scikit-learn implements many algorithms we've seen in this class such as:\n",
    "- Nearest neighbors\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "\n",
    "\n",
    "In scikit-learn, these algorithms are called **estimators**, and they use a uniform programming approach, which makes it very easy to switch to a new model or algorithm. An estimator is what we referred to as a **predictor** in our course. It specifies the function $f$ that maps a given feature $x \\in \\mathbb{R}^d$ to its associated label $f(x) \\in \\mathbb{R}$. \n",
    "\n",
    "Here is an example of many of the estimators available with scikit-learn. [Source](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a7823",
   "metadata": {},
   "source": [
    "Here are the steps to follow when using the scikit-learn estimator API:\n",
    "1. Arrange data into a features matrix (`X`) and target vector (`y`).\n",
    "2. Choose a class of model by importing the appropriate estimator class (e.g. `linear_model.LogisticRegression()`, `svm.SVC()`, etc...)\n",
    "3. Choose model hyperparameters by instantiating this class with desired values.\n",
    "4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
    "5. Apply the model to new data: for supervised learning, we predict labels for unknown data using the `predict()` method.\n",
    "\n",
    "The steps to follow when using scikit-learn estimators for unsupervised learning are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531512d",
   "metadata": {},
   "source": [
    "**Question**: Take a look at the first two datasets shown on the left of rows 1 and 2 above? Note that these are test datasets and the accuracy with resepct to this dataset is written. Which approaches have the best performance in the first row? What about in the second row? Notice that we have not covered all these approaches. Don't worry about it. Simply take this as an output given to you and try to analyze the output. How do you expect logistic regression would perform for these two datasets? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca90c47",
   "metadata": {},
   "source": [
    "**Answer**: For the first row data set, Nearest neighbor, RBF SVM and Gaussian process achieve the highest accuracy of 97%. For the second row data set, Nearest neighbor approach achieves the highest accuracy of 90%. The data does not look linearly separable. Since logistic regression (without any nonlinear transformation of features), separates the data using a affine function, we don't expect it to perform too well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1342471",
   "metadata": {},
   "source": [
    "### 1.3. Example - Logistic regression on the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592a62d",
   "metadata": {},
   "source": [
    "As an example, we'll walk through how to use scikit-learn to train a logistic regression model for multi-class classification on the Iris dataset. You can simply run the following cells and there is **no need to write any code here**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", context=\"notebook\", palette=\"dark\")\n",
    "# !!! sklearn is how the scikit-learn package is called in Python\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7b7c2",
   "metadata": {},
   "source": [
    "#### 1.3.1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a634f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Iris is a dataset , which is directly available in sklearn.datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2] # we only take the first two features for simpler visualizations\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Type of X: {type(X)} | Shape of X: {X.shape}\")\n",
    "print(f\"Type of y: {type(y)} | Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9872c",
   "metadata": {},
   "source": [
    "####  1.3.2. Splitting  and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd60475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data using train_test_split, use 30% of the data as a test set and set a random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape} | Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape} | Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5c8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit with the mean / std of the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale both the training / test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Mean of X_train: {X_train.mean():.3f}| Std of X_train: {X_train.std():.3f}\")\n",
    "print(f\"Mean of X_test: {X_test.mean():.3f}| Std of X_test: {X_test.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c9c84",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question**: What does the function StandardScalar() above do? You can read about standardization in sklearn [Here](https://scikit-learn.org/stable/modules/preprocessing.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac7417",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4debf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a logistic regression model with L2 regularization \n",
    "# and regularization strength 1e-4 ( C is inverse of regularization weight)\n",
    "logreg = LogisticRegression(penalty=\"l2\", C=1e4)\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get train accuracy\n",
    "train_acc = logreg.score(X_train, y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc9ce2",
   "metadata": {},
   "source": [
    "#### 1.3.4. Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8af40",
   "metadata": {},
   "source": [
    "We can use matplotlib to view the decision boundaries of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is beyond the scope of this class, no need to understand what it does.\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto', alpha=0.1, antialiased=True)\n",
    "\n",
    "# Plot also the training points\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(iris.target_names))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d91ddb",
   "metadata": {},
   "source": [
    "#### 1.3.5. Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "test_acc = logreg.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22c0a4",
   "metadata": {},
   "source": [
    "#### 1.3.6. Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily use other metrics using sklearn.metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# First we'll use the balanced accuracy\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "train_balanced_acc = balanced_accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train balanced acc: {train_balanced_acc*100:.2f}%\")\n",
    "print(f\"Test balanced acc: {test_balanced_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e6bc",
   "metadata": {},
   "source": [
    "**Question:** Under which condition on data labels would the balanced accuracy be the same as the accuracy computed originally? You may read about balanced_accuracy_score [Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334dc17",
   "metadata": {},
   "source": [
    "Recall that we saw the confusion matrix in a binary classification setting. \n",
    "For $K$ classes (also referred to as categories), the confusion matrix $C$ is a $K$ by $K$ matrix, where\n",
    "the diagonal element $C_{ii}$ denotes the number of correctly labelled class $i$ samples. The off-diagonal entri $C_{ij}$ denotes the number of class $j$ samples that are labelled mistakenly as class $i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b78119",
   "metadata": {},
   "source": [
    "**Question:** based on the accuracy score above, and given that the test dataset has 45 samples, what would be the sum of diagonal entries of the confusion matrix for this particular probnlem? What about the sum of off-diagonal entries? You can verify your answers with the matrix below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6f286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, logreg.predict(X_test), labels=[0,1,2]), display_labels=iris.target_names)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6e8bd",
   "metadata": {},
   "source": [
    "### 1.4 A note on unsupervised learning: \n",
    "While we won't cover them in this exercise, most of the unsupervised learning techniques seen in class can be easily implemented with scikit-learn.\n",
    "\n",
    "To learn more about how to practically implement these techniques, check out these resources:\n",
    "\n",
    "**For dimensionality reduction:**\n",
    "- [PCA from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)\n",
    "- [Decomposition page on scikit-learn's website](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "\n",
    "**For clustering:**\n",
    "- [k-means from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "- [Clustering page on scikit-learn's website](https://scikit-learn.org/stable/modules/clustering.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144552b7",
   "metadata": {},
   "source": [
    "### 1.5. Additional scikit-learn resources\n",
    "\n",
    "This tutorial very briefly covers the scikit-learn package, and how it can be used to train a simple classifier. This package is capable of a lot more than what was shown here, as you will see in the rest of this exercise. If you want a more in-depth look at scikit-learn, take a look at these resources:\n",
    "\n",
    "- scikit-learn Getting Started tutorial: https://scikit-learn.org/stable/getting_started.html\n",
    "- scikit-learn User Guide: https://scikit-learn.org/stable/user_guide.html\n",
    "- scikit-learn cheatsheet by Datacamp: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf\n",
    "- scikit-learn tutorial from the Python Data Science Handbook: https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ceada",
   "metadata": {},
   "source": [
    "## 2. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457089be",
   "metadata": {},
   "source": [
    "Decision trees are a very intuitive way to classify objects: they ask a series of questions to infer the target variable. \n",
    "\n",
    "A decision tree is a set of nested decision rules. At each node $i$, the $d_i$-th feature of the input vector $ \\boldsymbol{x}$ is compared to a treshold value $t$. The vector $\\boldsymbol{x}$ is passed down to the left or right branch depending on whether $d_i$ is less than or greater than $t$. This process is repeated for each node encountered until a reaching leaf node, which specifies the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff4ae0",
   "metadata": {},
   "source": [
    "<img src=\"images/simple_tree.png\" width=400></img>\n",
    "\n",
    "*Example of a simple decision tree on the Palmer Penguins dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd0620",
   "metadata": {},
   "source": [
    "Decision trees are usually constructed from the top-down, by choosing a feature at each step that best splits the set of items. The problem of choosing the best feature and threshold is a very challenging optimization problem. Hence, there has been a number of different criteria for measuring the \"best\" feature to pick, such as the Gini impurity and the entropy / information gain. We won't dive into them in this course. You should just understand that there are different ways to measure \"best\" and often algorithms provide an approximation of the \"optimal\" decision tree. For further information, you may see StatQuest videos on decision trees and you can also see Chapter 18 of [\"Probabilistic Machine Learning: An Introduction\"](https://probml.github.io/pml-book/) by K.P. Murphy.\n",
    "\n",
    "Decision trees are popular for several reasons:\n",
    "- They are **easy to interpret**.\n",
    "- They can handle mixed discrete and continuous inputs. \n",
    "- They are insensitive to monotone transformations of the inputs, so there is no need to standardize the data.\n",
    "- They perform automatic feature selection.\n",
    "- They are fast to ﬁt, and scale well to large data sets.\n",
    "\n",
    "Unfortunately, trees usually do not predict as accurately as other models we have seen previously, such as neural networks and SVMs.\n",
    "\n",
    "It is however possible to significantly improve their performance through an ensemble learning method called **random forests**, which consists of constructing a multitude of decision trees at training time and averaging their outputs at test time. While random forests usually perform better than a single decision tree, they are much less interpretable. We won't cover random forests in this exercise, but keep in mind that they can be easily implemented in scikit-learn using the [`ensemble` module](https://scikit-learn.org/stable/modules/ensemble.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053414c3",
   "metadata": {},
   "source": [
    "### 2.1. Training  decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f91fd7",
   "metadata": {},
   "source": [
    "In this part, we will work on the Titanic dataset obtained at the end of the `08-pandas` exercise. Notice that going through the `08-pandas` is not required but can help you understand the dataset better. In particular, the data set contains the following entries in its columns:\n",
    "- **survived**: Survival of the passenger (0 = No, 1 = Yes)\n",
    "- **pclass**: Ticket class (1= 1st, 2 = 2nd, 3 = 3rd)\n",
    "- **age**: Age\n",
    "- **sibsp**: # of siblings / spouses aboard the Titanic\n",
    "- **parch**: # of parents / children aboard the Titanic\n",
    "- **fare**: Passenger fare\n",
    "- **alone**: Whether the passenger is alone or not\n",
    "- **sex**: Sex in {female, male} (2 columns one-hot)\n",
    "- **embark_town**: Port of embarkation in {Southampton, Cherbourg, Queenstown} (3 columns one-hot)\n",
    "- **who**: Child, man, or woman (3 columns one-hot)\n",
    "- **fare_group**: Fare group in {(-0.001,7.854], (7.854, 10.5], (10.5, 21.679], (21.679, 39.688], (39.688, 512.329]} (5 columns one-hot)\n",
    "\n",
    "Note, that some of the features here differ from those in exercise 08-pandas. In the following, our goal is to train a model to predict whether or not a passenger survived the shipwreck and to find out which features are the most useful for predicting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9dd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = titanic.drop(columns=\"survived\")\n",
    "y = titanic[\"survived\"]\n",
    "\n",
    "# Convert to NumPy (needed for interpretability function later on)\n",
    "X_numpy, y_numpy = X.to_numpy(), y.to_numpy()\n",
    "\n",
    "# Use 80% of data for train/val, 20% for test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, random_state=42)\n",
    "# Use 80% of trainval for train, 20% for val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa096d",
   "metadata": {},
   "source": [
    "It is now your turn to train decision trees in scikit-learn. They follow the same estimator API as all other supervised learning models, so the implementation is very straightforward. For more information, check out the [`tree` module](https://scikit-learn.org/stable/modules/tree.html#tree) and the [documentation of `DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "**Your task:** Initialize a `DecisionTreeClassifier` and train it on `X_train` and `y_train`.\n",
    "For the initialization of the `DecisionTreeClassifier`, provide the following optional arguments:\n",
    "- Use \"gini\" as the `criterion`\n",
    "- Try out different values for the `max_depth` of the tree. How does it affect the train and validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d21f89",
   "metadata": {},
   "source": [
    "**Question:** What fraction of the original data set is used for validation? Why is the test set not used for validation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train your decision tree model\n",
    "# Try varying the max_depth parameter\n",
    "max_depth = 3\n",
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "train_acc = model.score(X_train, y_train)\n",
    "print(f\"Train accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "val_acc = model.score(X_val, y_val)\n",
    "print(f\"Validation accuracy: {val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d03f46",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e828782",
   "metadata": {},
   "source": [
    "### 2.2. Interpretability of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498485ca",
   "metadata": {},
   "source": [
    "In this section, we'll show you how to visualize decision trees and interpret the decision made for some examples of our test set.\n",
    "\n",
    "**Your task:** Run the next few cells to better understand the structure of the tree you just built. Can you identify which features are the most important for predicting whether or not a passenger survived? Note that some data point is passed to the left of some non-leaf node if the condition corresponding to the node is true and to the right if it is false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d81e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots the decision tree\n",
    "# Try out a max plot depth of 2 or 3, tree will be hard to read otherwise\n",
    "plt.figure(figsize=(30, 10))\n",
    "tree.plot_tree(model,  max_depth=max_depth, filled=True, feature_names=X.columns, class_names=[\"Perished\", \"Survived\"], \n",
    "               impurity=True, proportion=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the decision tree as text\n",
    "# Will be very long if max depth is high\n",
    "# Class 0 = Perished, Class 1 = Survived\n",
    "print(tree.export_text(model, feature_names=list(X.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining the decisions (complicated code, no need to understand what it does exactly)\n",
    "def explain_decision(sample_id: int = 0):\n",
    "    \"\"\"Prints rules followed to obtain prediction for a sample of the test set\n",
    "    \n",
    "    Code adapted from: \n",
    "    https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "    \"\"\"\n",
    "    sample_id = sample_id\n",
    "    class_names=[\"Perished\", \"Survived\"]\n",
    "    n_nodes = model.tree_.node_count\n",
    "    children_left = model.tree_.children_left\n",
    "    children_right = model.tree_.children_right\n",
    "    feature = model.tree_.feature\n",
    "    threshold = model.tree_.threshold\n",
    "\n",
    "    node_indicator = model.decision_path(X_test)\n",
    "    leaf_id = model.apply(X_test)\n",
    "    # obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "    prediction = class_names[model.predict(X_test[sample_id:sample_id+1])[0]]\n",
    "    print(f\"Prediction for sample {sample_id}: {prediction}\\n\")\n",
    "    print(\"Rules used:\")\n",
    "    for node_id in node_index:\n",
    "        # continue to the next node if it is a leaf node\n",
    "        if leaf_id[sample_id] == node_id:\n",
    "            continue\n",
    "\n",
    "        # check if value of the split feature for sample 0 is below threshold\n",
    "        if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "\n",
    "        print(\"- node {node}: ({feature} = {value}) \"\n",
    "              \"{inequality} {threshold}\".format(\n",
    "                  node=node_id,\n",
    "                  feature=X.columns[feature[node_id]],\n",
    "                  value=X_test[sample_id, feature[node_id]],\n",
    "                  inequality=threshold_sign,\n",
    "                  threshold=threshold[node_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary variables, 1 = True, 0 = False\n",
    "# e.g. sex_male = 1 -> male, sex_male = 0 -> female\n",
    "# Many of the features are redundant (e.g. sex_male and sex_female)\n",
    "# so the tree doesn't always choose the same features \n",
    "\n",
    "explain_decision(sample_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82113c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_decision(sample_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d5725",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b6c19",
   "metadata": {},
   "source": [
    "**To go further:** Decision trees and random forests can also be used for regression, check out the scikit-learn pages on [trees](https://scikit-learn.org/stable/modules/tree.html#tree) and [ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#ensemble) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d47e5",
   "metadata": {},
   "source": [
    "## 3. A small note on unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead51ad0",
   "metadata": {},
   "source": [
    "While we won't cover them in this exercise, most of the unsupervised learning techniques seen in class can be easily implemented with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f9020",
   "metadata": {},
   "source": [
    "As an example, here is how to use the k-means clustering algorithm on a toy dataset consisting of 7 unlabeled blobs of points. \n",
    "When choosing $k=7$, k-means manages to almost perfectly recover the original blobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate unlabeled data\n",
    "X_blobs, _ = make_blobs(n_samples=200, centers=7, random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=\"grey\", alpha=0.5)\n",
    "plt.title(\"Unlabeled data\")\n",
    "plt.show()\n",
    "\n",
    "# Run k-means on data to find the blobs\n",
    "\n",
    "# Try changing the value of k\n",
    "k = 7\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "# Display clusters and their centers\n",
    "plt.scatter(X_blobs[:,0], X_blobs[:,1], c=labels, cmap=\"viridis\", alpha=0.5)\n",
    "for c in kmeans.cluster_centers_:\n",
    "        plt.scatter(c[0], c[1], marker=\"*\", s=80, color=\"blue\")\n",
    "plt.title(f\"K-Means with {k} clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf1e62",
   "metadata": {},
   "source": [
    "To learn more about how to practically implement these techniques, check out these resources:\n",
    "\n",
    "**For dimensionality reduction:**\n",
    "- [PCA from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)\n",
    "- [Manifold learning from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html)\n",
    "- [Decomposition page on scikit-learn's website](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "- [Manifold learning page on scikit-learn's website](https://scikit-learn.org/stable/modules/manifold.html)\n",
    "\n",
    "**For clustering:**\n",
    "- [k-means from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "- [Gaussian mixtures from the Python Data Science handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
    "- [Clustering page on scikit-learn's website](https://scikit-learn.org/stable/modules/clustering.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0242cd",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "Using scikit-learn and several other python packages and libraries can help save your time in writing your own code for various machine learning algorithms. However, keep in mind that to properly use and interpret the results, you need to understand the mathematics used to implement various functions (loss functions, predictors, regularizers) and training algorithms (optimization approaches such as gradient descent or Newton methods) and accuracy metrics (e.g. accuracy, balanced accuracy). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
