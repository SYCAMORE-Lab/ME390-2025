{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "**Author:** Sabri El Amrani\n",
    "\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align all tables to the left (useful for later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Any, Callable\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Helper file with functions for pre-processing and visualization\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro\n",
    "\n",
    "Let's continue last week's notebook on the identification of species of plants using reflectance spectra. Last week, we used PCA to help reduce the dimensionality of these spectra. Let's now implement k-means to find out how these datapoints naturally cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the data we prepared last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle arrays arrays created in the pca notebook, similar to PCA, we use all data for training\n",
    "with open('data/pca_preprocessed_angers_dataset.pkl', 'rb') as f:\n",
    "    X_train, y_train, label_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to remember what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Map from label to label name: {label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shapes\n",
    "print('Training set shape:')\n",
    "print(f'X: {X_train.shape}, y: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by keeping the first two principal components for the sake of this exercice. We will see whether they suffice to properly cluster our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,:2]\n",
    "\n",
    "print('Training set shape:')\n",
    "print(f'X: {X_train.shape}, y: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data visualization & scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's scale our data as we often do in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Compute the mean and standard deviation for each feature of the training set\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Implement the normalize function\n",
    "def normalize(X: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
    "    \"\"\" Normalization of array using Z-score standardization\n",
    "     Args:\n",
    "        X: Dataset of shape (N, D)\n",
    "        mean: Mean of shape (D, )\n",
    "        std: Standard deviation of shape(D, )\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    X_normalized = (X - mean) / std\n",
    "    ### END CODE HERE ###\n",
    "    return X_normalized\n",
    "\n",
    "# Normalize features of the training, val and test set using the mean and std of the training set features\n",
    "X_train = normalize(X_train, mean, std)\n",
    "\n",
    "# Let's rename the features to indicate that they've been normalized\n",
    "feature_names = ['X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data using the `plot_labeled()` and `plot_unlabeled()` functions of `helpers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized training and testing data\n",
    "\n",
    "### START CODE HERE ###\n",
    "helpers.plot_labeled(X_train, y_train, label_map, feature_names, title=\"Training data\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does our data naturally form clusters? Do these clusters match the underlying species? Hard to say, so let's use k-means to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder of the algorithm:\n",
    "\n",
    "1. __Initialization:__ Start by randomly selecting K points from the dataset. These points will act as the initial cluster centers.\n",
    "2. __Assignment:__ For each data point in the dataset, calculate the distance between that point and each of the K centers. Assign the data point to the cluster whose center is closest to it. This step effectively forms K clusters.\n",
    "3. __Update centers:__ Once all data points have been assigned to clusters, recalculate the centers of the clusters by taking the mean of all data points assigned to each cluster.\n",
    "4. __Repeat:__ Repeat steps 2 and 3 until convergence. Convergence occurs when the centers no longer change significantly or when a specified number of iterations is reached.\n",
    "5. __Final Result:__ Once convergence is achieved, the algorithm outputs the final cluster centers and the assignment of each data point to a cluster.\n",
    "\n",
    "[Source](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the various steps one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centers(data, k):\n",
    "    \"\"\"\n",
    "    Selects k random points from the dataset as initial cluster centers.\n",
    "\n",
    "    Parameters:\n",
    "    data (ndarray): The dataset from which to select initial centers.\n",
    "    k (int): The number of centers to initialize.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: An array of k initial centers randomly chosen from the dataset.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    indices = np.random.choice(data.shape[0], k, replace=False)\n",
    "    return data[indices]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Assign data points to nearest center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the euclidean distance.\n",
    "\n",
    "__Hint:__ Don't forget to use broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(data, centers):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between each data point and each center.\n",
    "\n",
    "    Parameters:\n",
    "    data (ndarray): The dataset of shape (n_samples, n_features).\n",
    "    centers (ndarray): The centers of shape (k, n_features).\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A distance matrix of shape (k, n_samples) where each entry (i, j)\n",
    "             is the distance between the ith center and the jth data point.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    distances = np.sqrt(((data - centers[:, np.newaxis])**2).sum(axis=2))\n",
    "    return distances\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assign each datapoint to the nearest center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(data, centers, distance_fn):\n",
    "    \"\"\"\n",
    "    Assigns each data point to the nearest center.\n",
    "\n",
    "    Parameters:\n",
    "    data (ndarray): The dataset of shape (n_samples, n_features).\n",
    "    centers (ndarray): The centers of shape (k, n_features).\n",
    "    distance_fn (function): A function to compute distances between data points and centers.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: An array of shape (n_samples,) where each value is the index of the nearest center.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    distances = distance_fn(data, centers)\n",
    "    return np.argmin(distances, axis=0)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Update centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated center: mean of the assigned data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centers(data, labels, k):\n",
    "    \"\"\"\n",
    "    Updates center positions by calculating the mean of data points assigned to each center.\n",
    "\n",
    "    Parameters:\n",
    "    data (ndarray): The dataset of shape (n_samples, n_features).\n",
    "    labels (ndarray): An array of shape (n_samples,) containing cluster assignments.\n",
    "    k (int): The number of centerss or clusters.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: An array of shape (k, n_features) with the updated center positions.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
    "    return new_centers\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Check convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the centers have not moved significantly since the previous iteration. In other words, ensuring that no center has changed by more than a specified minimum value, referred to as the tolerance (a hyperparameter set to 1e-5 in this case).\n",
    "\n",
    "Hint: Use np.all() to check whether all elements in an array are less than the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_converged(old_centers, new_centers, tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Checks if the center positions have converged.\n",
    "\n",
    "    Parameters:\n",
    "    old_centers (ndarray): The previous center positions.\n",
    "    new_centers (ndarray): The updated center positions.\n",
    "    tolerance (float, optional): The convergence threshold. Default is 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all center changes are within the tolerance, otherwise False.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    return np.all(np.abs(old_centers- new_centers) < tolerance)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 K-Means algorithm: everything combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to stop the algorithm if either of these two conditions is met:\n",
    "1. The algorithm has converged.\n",
    "2. The maximum number of iterations has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T20:27:42.591987Z",
     "iopub.status.busy": "2024-11-24T20:27:42.589214Z",
     "iopub.status.idle": "2024-11-24T20:27:42.614842Z",
     "shell.execute_reply": "2024-11-24T20:27:42.612593Z",
     "shell.execute_reply.started": "2024-11-24T20:27:42.591892Z"
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k, distance_fn, max_iters=100, tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Runs the K-Means clustering algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    data (ndarray): The dataset of shape (n_samples, n_features).\n",
    "    k (int): The number of clusters.\n",
    "    distance_fn (function): A function to compute distances between data points and centers.\n",
    "    max_iters (int, optional): The maximum number of iterations to run the algorithm. Default is 100.\n",
    "    tolerance (float, optional): The convergence threshold. Default is 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - ndarray: The final center positions of shape (k, n_features).\n",
    "        - ndarray: The cluster labels for each data point, of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Initialize centers\n",
    "    centers = initialize_centers(data, k)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Step 2: Assign clusters\n",
    "        labels = assign_clusters(data, centers, distance_fn)\n",
    "        \n",
    "        # Step 3: Update centers\n",
    "        new_centers = update_centers(data, labels, k)\n",
    "        \n",
    "        # Step 4: Check for convergence\n",
    "        if has_converged(centers, new_centers, tolerance):\n",
    "            break\n",
    "        \n",
    "        centers = new_centers\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our algorithm the the our reduced Angers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters, try 2, 3 and 4\n",
    "k = 3\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Run K-Means algorithm\n",
    "centers, labels = kmeans(X_train, k, euclidean_dist)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i: f'Cluster {i}' for i in range(k)}\n",
    "\n",
    "helpers.plot_labeled(X_train, labels, label_map, feature_names, title=\"Training data - clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the clustering performance depends heavily on the initialization of the cluster centers. You can play with the random seed at the beginning of the kmeans(.) function. Check how clustering performs with different initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting hyperparameters (bonus reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the number of clusters, we can consider using the two following methods [(Source)](https://medium.com/@zalarushirajsinh07/the-elbow-method-finding-the-optimal-number-of-clusters-d297f5aeb189):\n",
    "1. Elbow method: we are looking for an inflection point in the plot of the sum of squared distances, as shown in the picture below.\n",
    "2. Silhouette score: the optimal k should correspond to the maximum of the silhouette plot.\n",
    "\n",
    "<img src=\"images/elbow.png\" style=\"width:500px\"/>\n",
    "\n",
    "Combine the insights of these two plots to determine the optimal number of clusters k.\n",
    "Given the small size of the dataset, we didn't split it into training/validation/test sets. Therefore, we will use the [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) (cf. logistic regression notebook) method on the training set to find the optimal k.\n",
    "\n",
    "That concludes this tutorial on k-means. Thank you for taking part!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
