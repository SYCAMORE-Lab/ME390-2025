{"cells":[{"cell_type":"markdown","id":"absolute-assignment","metadata":{"id":"absolute-assignment"},"source":["# k-Nearest Neighbors - Solutions"]},{"cell_type":"markdown","id":"answering-method","metadata":{"id":"answering-method"},"source":["<hr style=\"clear:both\">\n","\n","This notebook was developed for the CS-233 Introduction to Machine Learning course at EPFL, adapted for the CIVIL-226 Introduction to Machine Learning for Engineers course, and re-adapted for the ME-390.\n","We thank contributers in CS-233 ([CVLab](https://www.epfl.ch/labs/cvlab)) and CIVIL-226 ([VITA](https://www.epfl.ch/labs/vita/)).\n","    \n","**Author(s):** Jan Bednarík, minor changes by Tom Winandy\n","\n","**Adapted by:** Sabri El Amrani\n","<hr style=\"clear:both\">"]},{"cell_type":"markdown","id":"considered-stopping","metadata":{"id":"considered-stopping"},"source":["In this exercise, you will implement a k-Nearest Neighbors (k-NN) classifier and see how to use a validation set to find optimal hyper-parameters."]},{"cell_type":"code","execution_count":null,"id":"alien-jewel","metadata":{"id":"alien-jewel"},"outputs":[],"source":["# Function to align all tables to the left (useful for later on)"]},{"cell_type":"code","execution_count":null,"id":"historic-agent","metadata":{"id":"historic-agent","outputId":"7eabf8f7-81e9-46a4-9885-c657e084ffcf"},"outputs":[],"source":["%%html\n","<style>\n","table {float:left}\n","</style>"]},{"cell_type":"markdown","id":"genetic-operations","metadata":{"id":"genetic-operations"},"source":["### Imports"]},{"cell_type":"code","execution_count":6,"id":"common-immigration","metadata":{"id":"common-immigration"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from typing import Any, Callable\n","\n","# Helper file with functions for pre-processing and visualization\n","import helpers"]},{"cell_type":"markdown","id":"rational-karma","metadata":{"id":"rational-karma"},"source":["## 0. Intro\n","\n","*__Background:__*\n","*As an engineer tasked with enhancing diagnostic accuracy and treatment strategies in orthopedic care, you're analyzing a dataset of biomechanical features from patients with different spinal conditions. Your goal is to develop a predictive model that accurately classifies patients based on these features. This classification task is crucial for improving the diagnosis and treatment plans for patients, thereby ensuring more personalized and effective orthopedic care.*\n","\n","<img src=\"images/herniated_disk.png\" alt=\"hernia\" style=\"width:500px\"/>\n","\n","[Source](https://www.mayoclinic.org/diseases-conditions/herniated-disk/symptoms-causes/syc-20354095#dialogId24124081)\n","\n","To build such a classifier, you will need some labeled data. Luckily for you, our dataset contains exactly the data you need.\n","\n","You decide to implement a kNN classifier. As a reminder, it works in the following way: For an unknown data point, find the k nearest neighbors from a labeled dataset, and return the majority vote (most frequent class) of these neighbors."]},{"cell_type":"markdown","id":"convenient-circumstances","metadata":{"id":"convenient-circumstances"},"source":["## 1. Data loading & pre-processing"]},{"cell_type":"markdown","id":"hungarian-favorite","metadata":{"id":"hungarian-favorite"},"source":["In this exercise, you will use a simplified version of the [Biomechanical features of orthopedic patients](https://www.kaggle.com/datasets/uciml/biomechanical-features-of-orthopedic-patients) and your goal is to classify patients into one of three categories: Normal (100 patients), Disk Hernia (60 patients), or Spondylolisthesis (150 patients). The two features at your disposal are the pelvic tilt (°) and the degree of spondylolisthesis (°).\n","\n","Let's load the dataset using the [pandas](https://pandas.pydata.org/) library. Don't worry, there is as usual no need to know how to use this library for this exercise."]},{"cell_type":"code","execution_count":null,"id":"tough-airport","metadata":{"id":"tough-airport","outputId":"425258ea-2924-401b-8abe-40866ead1548"},"outputs":[],"source":["# In Pandas, a data table is called a DataFrame (abbreviated to df)\n","labeled_df = pd.read_csv('data/biomech_3C.csv')\n","\n","print(f\"There are {labeled_df.shape[0]} rows and {labeled_df.shape[1]} columns.\")\n","# Show the first 5 rows of the data\n","labeled_df.head(5)"]},{"cell_type":"markdown","id":"political-consumer","metadata":{"id":"political-consumer"},"source":["Now, we'll use the function `preprocess_data()` from `helpers.py` to shuffle the data, convert the labels to integers, split it into a training, validation and test set, and then return the result as NumPy arrays.\n","Feel free to read the code in `helpers.py` to better understand what this function does (completely optional)."]},{"cell_type":"code","execution_count":8,"id":"passive-title","metadata":{"id":"passive-title"},"outputs":[],"source":["# We'll use 60% of our data as training data, 20% for our validation data and the remaining 20% as test data\n","# Here, we use a random seed to ensure that the data shuffling and splitting can be reproduced\n","X_train, y_train, X_val, y_val, X_test, y_test, feature_names, label_map = helpers.preprocess_data(df=labeled_df, label=\"class\", train_size=0.6, val_size=0.2, seed=42)"]},{"cell_type":"markdown","id":"willing-sierra","metadata":{"id":"willing-sierra"},"source":["Run the following cells to better understand what the preprocessed data looks like."]},{"cell_type":"code","execution_count":null,"id":"historical-colorado","metadata":{"id":"historical-colorado","outputId":"086941d4-ff70-4f42-d6ac-d82a2d0ee786"},"outputs":[],"source":["print(f\"Features: {feature_names}\")\n","print(f\"Map from label to label name: {label_map}\")"]},{"cell_type":"code","execution_count":null,"id":"renewable-teens","metadata":{"id":"renewable-teens","outputId":"737b5342-71b9-4274-a5c4-e86608e13d82"},"outputs":[],"source":["# Visualisation of X_train and y_train (separation of the features and the labels)\n","print('Training set features (first 10 samples):')\n","print(f'X_train: \\n {X_train[:10]}')\n","\n","print('\\nTraining set labels (first 10 samples):')\n","print(f'y_train: \\n {y_train[:10]}')"]},{"cell_type":"code","execution_count":null,"id":"checked-judges","metadata":{"id":"checked-judges","outputId":"0679fee6-c102-409c-bef8-0aad8648cce5"},"outputs":[],"source":["# Show shapes\n","print('Training set shape:')\n","print(f'X: {X_train.shape}, y: {y_train.shape}')\n","\n","print('\\nValidation set shape:')\n","print(f'X: {X_val.shape}, y: {y_val.shape}')\n","\n","print('\\nTest set shape:')\n","print(f'X: {X_test.shape}, y: {y_test.shape}')"]},{"cell_type":"markdown","id":"incorporate-holmes","metadata":{"id":"incorporate-holmes"},"source":["## 2. Data visualization & scaling"]},{"cell_type":"markdown","id":"auburn-bleeding","metadata":{"id":"auburn-bleeding"},"source":["Let's take a look at our data using the `plot_labeled()` and `plot_unlabeled()` functions of `helpers.py`."]},{"cell_type":"code","execution_count":null,"id":"palestinian-oliver","metadata":{"id":"palestinian-oliver","outputId":"5a946fd7-1e85-400a-e1c1-bfa0c28ca167"},"outputs":[],"source":["# Plot training data with labels\n","helpers.plot_labeled(X_train, y_train, label_map, feature_names, title=\"Training data\")\n","# Plot validation data with labels\n","helpers.plot_labeled(X_val, y_val, label_map, feature_names, title=\"Validation data\")\n","# Plot test data with labels\n","helpers.plot_labeled(X_test, y_test, label_map, feature_names, title=\"Test data\")"]},{"cell_type":"markdown","id":"weekly-clinic","metadata":{"id":"weekly-clinic"},"source":["**Question:** What do you notice? How will this affect the kNN classifier?"]},{"cell_type":"markdown","id":"material-norway","metadata":{"id":"material-norway"},"source":["**Answer:** The pelvic tilt and the degree of spondylolisthesis features have different scales. As kNN relies on distance for classification, if these scales are kept, then the pelvic tilt feature will have much less impact on the classifier, which can considerably impact our model's accuracy."]},{"cell_type":"markdown","id":"interesting-burden","metadata":{"id":"interesting-burden"},"source":["As features can represent different physical units and come in vastly different scales, it is common practice to normalize the  data before using kNN.\n","\n","Here, we'll use Z-score standardization.  That is, for each feature, compute: $x_{norm} = \\frac {x - \\mu_x} {\\sigma_x}$\n","\n","Let's implement the function `normalize()`:\n","\n","**Note:** Remember that you should not use any of the knowledge you get from the test data when implementing a model. This includes the normalization step, where you should use the mean and standard deviation of the **training set** to normalize all the other datasets."]},{"cell_type":"code","execution_count":14,"id":"swiss-music","metadata":{"id":"swiss-music"},"outputs":[],"source":["# Compute the mean and standard deviation for each feature of the training set\n","\n","### START CODE HERE ### (≈ 2 lines of code)\n","mean = X_train.mean(axis=0)\n","std = X_train.std(axis=0)\n","### END CODE HERE ###\n","\n","\n","# Implement the normalize function\n","def normalize(X: np.ndarray, mean: np.ndarray, std: np.ndarray):\n","    \"\"\" Normalization of array using Z-score standardization\n","     Args:\n","        X: Dataset of shape (N, D)\n","        mean: Mean of shape (D, )\n","        std: Standard deviation of shape(D, )\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    X_normalized = (X - mean) / std\n","    ### END CODE HERE ###\n","    return X_normalized\n","\n","# Normalize features of the training, val and test set using the mean and std of the training set features\n","X_train = normalize(X_train, mean, std)\n","X_val = normalize(X_val, mean, std)\n","X_test = normalize(X_test, mean, std)\n","\n","# Let's rename the features to indicate that they've been normalized\n","feature_names = ['Pelvic tilt (normalized)', 'Degree of spondylolisthesis (normalized)']"]},{"cell_type":"code","execution_count":null,"id":"imported-addition","metadata":{"id":"imported-addition","outputId":"2aecd470-d8a8-4528-ee9f-b03837b7bbcc"},"outputs":[],"source":["# Verify that normalization is implemented correctly\n","# We check that the mean and std of each feature of the training set is very close to 0 and 1 respectively\n","if (np.allclose(X_train.mean(axis=0), np.zeros_like(X_train)) and (np.allclose(X_train.std(axis=0), np.ones_like(X_train)))):\n","    print(\"All good!\")\n","else:\n","    print(\"normalize() doesn't work correctly :(\")"]},{"cell_type":"markdown","id":"sixth-novelty","metadata":{"id":"sixth-novelty"},"source":["Now that the data is normalized, let's plot it again, using the same functions from `helpers` as before."]},{"cell_type":"code","execution_count":null,"id":"round-michael","metadata":{"id":"round-michael","outputId":"bd1a2cb3-0429-4235-ae3c-34e74fae4a78"},"outputs":[],"source":["# Plot normalized training data with labeled, and normalized test data without labels\n","helpers.plot_labeled(X_train, y_train, label_map, feature_names, title=\"Training data\")\n","helpers.plot_labeled(X_val, y_val, label_map, feature_names, title=\"Validation data\")\n","helpers.plot_labeled(X_test, y_test, label_map, feature_names, title=\"Test data\")"]},{"cell_type":"markdown","id":"general-package","metadata":{"id":"general-package"},"source":["This is much better, right? Now that the data is normalized, we can implement our classifier."]},{"cell_type":"markdown","id":"saving-township","metadata":{"id":"saving-township"},"source":["## 3. Distance metrics"]},{"cell_type":"markdown","id":"incoming-treasury","metadata":{"id":"incoming-treasury"},"source":["In this part, we'll work on the distance metrics used by the classifier. We have seen two distance metrics:\n","\n","**The L1 (Manhattan) distance:**\n","\n","$$\n","d(\\mathbf{p}, \\mathbf{q})=\\sum_{i=1}^{n} | p_{i}-q_{i} |\n","$$\n","\n","It is also sometimes known as city block or taxicab distance, in allusion to the fact that the shortest path by car in a city with a grid layout (such as Manhattan) would result in this distance.\n","\n","**The L2 (Euclidean) distance:**\n","\n","$$\n","d(\\mathbf{p}, \\mathbf{q})=\\sqrt{\\sum_{i=1}^{n}\\left(p_{i}-q_{i}\\right)^{2}}\n","$$\n","\n","This is the distance with which you are probably most familar with, it can be seen as the straight line distance between two points.\n","\n","![distances](images/manhattan_euclidean.png)\n"]},{"cell_type":"markdown","id":"affected-netscape","metadata":{"id":"affected-netscape"},"source":["Let's implement functions that compute the distance between a single vector to all vectors of the training data. Until the end of this exercise, we define \"sample\" as a single entry of the dataset. The entry is a vector of D attributes, and the code should stay flexible for any number of attributes.\n","\n","**Hint:** NumPy's [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is your friend."]},{"cell_type":"code","execution_count":17,"id":"amazing-frost","metadata":{"id":"amazing-frost"},"outputs":[],"source":["def manhattan_dist(sample: np.ndarray, X: np.ndarray):\n","    \"\"\"Computes the Manhattan distance between a sample and the training features\n","\n","    Args:\n","        sample: Sample of shape (D, )\n","        X: Dataset of shape (N, D)\n","\n","    Returns:\n","        dist: Distances of shape (N,)\n","\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    distances = np.abs(X - sample).sum(axis = 1)\n","    ### END CODE HERE ###\n","    return distances\n","\n","def euclidean_dist(sample: np.ndarray, X: np.ndarray):\n","    \"\"\"Computes the Euclidean distance between a sample and the training features\n","\n","    Args:\n","        sample: Sample of shape (D, )\n","        X: Dataset of shape (N, D)\n","\n","    Returns:\n","        dist: Distances of shape (N,)\n","\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    distances = np.sqrt(((X - sample) ** 2).sum(axis = 1))\n","    ### END CODE HERE ###\n","    return distances"]},{"cell_type":"code","execution_count":null,"id":"advised-complex","metadata":{"id":"advised-complex","outputId":"c2e6873b-d159-4a89-9c78-531acf04b061"},"outputs":[],"source":["# Verify implementation\n","print(f'Manhattan: {np.round(manhattan_dist(X_val[0], X_train[:3]), decimals=1)}')\n","print(f'Euclidean: {np.round(euclidean_dist(X_val[0], X_train[:3]), decimals=1)}')"]},{"cell_type":"markdown","id":"reserved-leeds","metadata":{"id":"reserved-leeds"},"source":["**Expected output:**  \n","\n","|   |                                                  |\n","|---|--------------------------------------------------|\n","| **Manhattan** | [1.4 0.9 0.3] |\n","| **Euclidean** | [1.4 0.8 0.2] |"]},{"cell_type":"markdown","id":"extensive-vampire","metadata":{"id":"extensive-vampire"},"source":["## 4. Finding neighbors"]},{"cell_type":"markdown","id":"numerical-rogers","metadata":{"id":"numerical-rogers"},"source":["It's time to find the nearest neighbors.\n","Implement `find_nearest_neighbors()` according to its documentation.\n","\n","**Hint:** [`np.argsort()`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) is a very useful function when working with indices."]},{"cell_type":"code","execution_count":19,"id":"rough-lawrence","metadata":{"id":"rough-lawrence"},"outputs":[],"source":["def find_nearest_neighbors(\n","    sample: np.ndarray,\n","    X: np.ndarray,\n","    distance_fn: Callable = euclidean_dist,\n","    k: int = 1):\n","    \"\"\"Finds the indices of the k-Nearest Neighbors to a sample\n","    Args:\n","        sample: Sample of shape (D, )\n","        X: Dataset of shape (N, D)\n","        distance_fn: Distance function\n","        k: Number of nearest neighbors\n","\n","    Returns:\n","        indices: Neighbor indices of shape (k, )\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    distances = distance_fn(sample, X)\n","    neighbor_indices = np.argsort(distances)[:k]\n","    ### END CODE HERE ###\n","\n","    return neighbor_indices"]},{"cell_type":"code","execution_count":null,"id":"representative-conflict","metadata":{"id":"representative-conflict","outputId":"c1bcbbf9-207b-49d6-ce8a-b9edd65f1025"},"outputs":[],"source":["# Try changing the sample position, distance function and value of k\n","# Does your find_nearest_neighbors() implementation seem correct?\n","sample = np.array([-0.5, -1.5])\n","distance_fn = euclidean_dist\n","k = 3\n","\n","# Plot the nearest neighbors using helpers.py\n","neighbor_indices = find_nearest_neighbors(sample, X_train, distance_fn=distance_fn, k=k)\n","helpers.plot_nearest_neighbors(sample, X_train, y_train, neighbor_indices, label_map, feature_names)"]},{"cell_type":"markdown","id":"neural-reply","metadata":{"id":"neural-reply"},"source":["The opaque points correspond to the nearest neighbors to the unknown point (star)."]},{"cell_type":"markdown","id":"pleased-party","metadata":{"id":"pleased-party"},"source":["Now that you have a function that finds the nearest neighbor, let's implement the function `predict_single()` which returns the majority class of the k-nearest neighbors of a given sample. Do not worry about tie-breakers (i.e. if multiple classes have as many points in proximity to a sample).\n","\n","**Hints:**\n","- Feel free to reuse functions you implemented previously\n","- Labels are numbered from 0 to n-1 (with n the number of classes)\n","- [`np.bincount()`](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html) and [`np.argmax()`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) can be very useful."]},{"cell_type":"code","execution_count":21,"id":"diagnostic-proportion","metadata":{"id":"diagnostic-proportion"},"outputs":[],"source":["def predict_single(\n","    sample: np.ndarray,\n","    X: np.ndarray,\n","    y: np.ndarray,\n","    distance_fn: Callable,\n","    k: int = 1):\n","    \"\"\" Finds the k-Nearest Neighbors to a sample and returns the majority class\n","    Args:\n","        sample: Sample of shape (D, )\n","        X: Dataset of shape (N, D)\n","        y: Labels of shape (N, )\n","        distance_fn: Distance function\n","        k: number of nearest neighbors\n","\n","    Returns:\n","        label: Predicted label, the majority class of the k-Nearest Neighbors\n","\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    # From your single sample, get the k closest neighbors among X, using your defined distance function:\n","    neighbor_indices = find_nearest_neighbors(sample, X, distance_fn, k)\n","    # You have the indice list among the dimension data, get their label from the label data:\n","    neighbor_labels = y[neighbor_indices]\n","    # Now count how many neighbors you have for each label:\n","    neighbor_label_count = np.bincount(neighbor_labels)\n","    # Get the indice of the most counted label. This is your KNN solution:\n","    predicted_label = np.argmax(neighbor_label_count)\n","    ### END CODE HERE ###\n","\n","    return predicted_label"]},{"cell_type":"markdown","id":"white-english","metadata":{"id":"white-english"},"source":["Now, we can reuse `predict_single()` to predict several samples at once.\n","\n","**Hint:** There are several ways to do this, such as using a simple for-loop or with the function [`np.apply_along_axis()`](https://numpy.org/doc/stable/reference/generated/numpy.apply_along_axis.html)"]},{"cell_type":"code","execution_count":22,"id":"hazardous-ability","metadata":{"id":"hazardous-ability"},"outputs":[],"source":["# With for-loop\n","def predict(\n","    samples: np.ndarray,\n","    X: np.ndarray = X_train,\n","    y: np.ndarray = y_train,\n","    distance_fn: Callable = euclidean_dist,\n","    k: int = 1):\n","    \"\"\" Finds the k-Nearest Neighbors to a matrix of samples and returns\n","        the majority class for each of these samples as an array\n","    Args:\n","        samples: Samples of shape (M, D)\n","        X: Dataset of shape (N, D)\n","        y: Labels of shape (N, )\n","        distance_fn: Distance function\n","        k: number of nearest neighbors\n","\n","    Returns:\n","        labels: Predicted labels of shape (M, )\n","\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 to 5 lines of code depending on the implementation)\n","    predicted_labels = []\n","    for i in range(samples.shape[0]):\n","        predicted_label = predict_single(samples[i, :], X, y, distance_fn, k)\n","        predicted_labels.append(predicted_label)\n","    # Convert to np.ndarray\n","    predicted_labels = np.array(predicted_labels)\n","     ### END CODE HERE ###\n","\n","    return predicted_labels\n","\n","# With apply-along-axis\n","def predict_1(\n","    samples: np.ndarray,\n","    X: np.ndarray = X_train,\n","    y: np.ndarray = y_train,\n","    distance_fn: Callable = euclidean_dist,\n","    k: int = 1):\n","    \"\"\" Finds the k-Nearest Neighbors to a matrix of samples and returns\n","        the majority class for each of these samples as an array\n","    Args:\n","        samples: Samples of shape (M, D)\n","        X: Dataset of shape (N, D)\n","        y: Labels of shape (N, )\n","        distance_fn: Distance function\n","        k: number of nearest neighbors\n","\n","    Returns:\n","        labels: Predicted labels of shape (M, )\n","\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 to 5 lines of code depending on the implementation)\n","    predicted_labels = np.apply_along_axis(predict_single, axis=1, arr=samples,\n","                                           X=X, y=y, distance_fn=distance_fn, k=k)\n","    ### END CODE HERE ###\n","\n","    return predicted_labels"]},{"cell_type":"code","execution_count":null,"id":"liable-haven","metadata":{"id":"liable-haven","outputId":"3ad5710a-b0c5-4cd5-cf7f-a0b96f889e1a"},"outputs":[],"source":["# Verify implementation\n","print(f'Predicted labels: {predict(X_val[:6], X_train, y_train, euclidean_dist, 5)}')"]},{"cell_type":"markdown","id":"connected-gilbert","metadata":{"id":"connected-gilbert"},"source":["**Expected output:**  \n","\n","|   |                                                  |\n","|---|--------------------------------------------------|\n","| **Predicted labels** | [1 1 2 1 2 1] |"]},{"cell_type":"markdown","id":"falling-lighter","metadata":{"id":"falling-lighter"},"source":["Let's visualize what predictions look like on our validation set.\n","In the following plot, stars represent the validation data (and the color represents their predicted class)."]},{"cell_type":"code","execution_count":null,"id":"turned-restriction","metadata":{"id":"turned-restriction","outputId":"0c5dde32-4244-4bb1-849b-efff111f13fd"},"outputs":[],"source":["# Try changing the distance function and value of k\n","distance_fn = euclidean_dist\n","k = 3\n","\n","# Plot the predictions using helpers.py\n","predicted_val_labels = predict(X_val, X_train, y_train, distance_fn, k=k)\n","helpers.plot_predictions(samples=X_val, predicted_labels=predicted_val_labels, X=X_train, y=y_train,\n","                         label_map=label_map, feature_names=feature_names,\n","                         title=\"Predicted class for validation set\")"]},{"cell_type":"markdown","id":"professional-spring","metadata":{"id":"professional-spring"},"source":["## 5. Selecting hyper-parameters"]},{"cell_type":"markdown","id":"atmospheric-peter","metadata":{"id":"atmospheric-peter"},"source":["Now is the time to select hyper-parameters. For kNN, the two parameters we can select is the distance function and the number of neighbors k."]},{"cell_type":"markdown","id":"executed-variety","metadata":{"id":"executed-variety"},"source":["First, we need a metric to measure how well our model is doing.\n","\n","Here, we'll use the accuracy, defined as:\n","\n","$$\n","\\text{accuracy} = \\frac{\\text{\\# correct predictions}}{\\text{\\# all predictions}}\n","$$\n","\n","Let's implement this function:"]},{"cell_type":"code","execution_count":25,"id":"precious-johns","metadata":{"id":"precious-johns"},"outputs":[],"source":["def accuracy(labels_gt: np.ndarray, labels_pred: np.ndarray):\n","    \"\"\" Computes accuracy.\n","\n","    Args:\n","        labels_gt: labels (ground-truth) of shape (M, ).\n","        labels_pred: Predicted labels of shape (M, ).\n","\n","    Returns:\n","        float: Accuracy, in range [0, 1].\n","    \"\"\"\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    acc = np.sum(labels_gt == labels_pred) / labels_gt.shape[0]\n","    # Alternative (nicer) formula:\n","    # acc = np.mean(labels_gt == labels_pred)\n","    ### END CODE HERE ###\n","    return acc"]},{"cell_type":"markdown","id":"familiar-nothing","metadata":{"id":"familiar-nothing"},"source":["Now, let's compute the accuracy for both distance functions at all values of k. We can only afford to do that because our dataset is relatively small, so this exhaustive search isn't too computationally expensive."]},{"cell_type":"code","execution_count":26,"id":"functioning-photography","metadata":{"id":"functioning-photography"},"outputs":[],"source":["k_values = []\n","euclidean_accs = []\n","manhattan_accs = []\n","for i in range(1, len(X_train) + 1):\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    manhattan_acc = accuracy(y_val, predict(X_val, distance_fn=manhattan_dist, k=i))\n","    euclidean_acc = accuracy(y_val, predict(X_val, distance_fn=euclidean_dist, k=i))\n","    ### END CODE HERE ###\n","    k_values.append(i)\n","    manhattan_accs.append(manhattan_acc)\n","    euclidean_accs.append(euclidean_acc)\n"]},{"cell_type":"code","execution_count":null,"id":"ranging-cartoon","metadata":{"id":"ranging-cartoon","outputId":"d2e067a7-3624-4119-fadd-7db597304037"},"outputs":[],"source":["# Plot accuracies\n","plt.figure(figsize=(8,4), dpi=100)\n","plt.plot(k_values, manhattan_accs, label=\"Manhattan\", alpha=0.8)\n","plt.plot(k_values, euclidean_accs, label=\"Euclidean\", alpha=0.8)\n","plt.xlabel(\"Value of k\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.title(\"Performance on the validation set for different values of $k$ and distance functions\")\n","plt.show()"]},{"cell_type":"markdown","id":"accurate-research","metadata":{"id":"accurate-research"},"source":["**Question:** What do you notice at the highest value of k? How can this be explained?"]},{"cell_type":"markdown","id":"modified-mount","metadata":{"id":"modified-mount"},"source":["**Answer:** The accuracy drops dramatically at the highest value of k. This is because, when k is equal to the number of examples in the training data, the kNN classifier returns the majority class of the entire dataset."]},{"cell_type":"markdown","id":"comparable-therapist","metadata":{"id":"comparable-therapist"},"source":["The classifier seems to perform well for both distance metrics and for values of k ranging from 1 to 50, but it is hard to pinpoint which value of k is optimal, and which distance metric is better. This is why, for small datasets, it is often preferred to use [K-Fold Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) instead of just a single validation set, as it provides more accurate results and is less sensitive to noise. For now, just select a value of k and a distance metric that provides good results according to the plot above."]},{"cell_type":"code","execution_count":28,"id":"expensive-nursery","metadata":{"id":"expensive-nursery"},"outputs":[],"source":["# Choose a good value of k and a good distance metric\n","k = 25\n","distance_fn = manhattan_dist"]},{"cell_type":"markdown","id":"unlikely-circulation","metadata":{"id":"unlikely-circulation"},"source":["Now that we selected  good hyperparameters, it is time to check how well our model performs on the test set."]},{"cell_type":"code","execution_count":null,"id":"continuing-favor","metadata":{"id":"continuing-favor","outputId":"52cb725d-b523-4e45-a0e1-de6fb5014800"},"outputs":[],"source":["# Compute the test accuracy\n","### START CODE HERE ### (≈ 1 line of code)\n","test_accuracy = accuracy(y_test, predict(X_test, distance_fn=distance_fn, k=k))\n","### END CODE HERE ###\n","print(f\"Test accuracy: {test_accuracy * 100: .2f}%\")"]},{"cell_type":"markdown","id":"2d3a5de2","metadata":{"id":"2d3a5de2"},"source":["Good job completing this notebook! Note that for the sake of visualization, we only selected a subset of the 6 features in the original [dataset](https://www.kaggle.com/datasets/uciml/biomechanical-features-of-orthopedic-patients). Don't hesitate to try to improve the model's accuracy by using the complete dataset, freely available for download on Kaggle."]},{"cell_type":"markdown","id":"61b58763","metadata":{"id":"61b58763"},"source":["In the next notebook, you will learn to use neural networks for classifying electrical faults in transmission lines and for predicting the electrical breakdown of insulators."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"toc-showcode":false,"toc-showmarkdowntxt":false,"toc-showtags":false},"nbformat":4,"nbformat_minor":5}
