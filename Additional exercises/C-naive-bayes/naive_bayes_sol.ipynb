{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "**Author:** Sabri El Amrani\n",
    "\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align all tables to the left (useful for later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro\n",
    "\n",
    "*__Background:__\n",
    "As a data analyst focused on optimizing smart home technologies, you're tasked with understanding the factors that contribute to the efficiency and performance of smart home devices. You are working with a dataset that captures various metrics related to device usage, energy consumption, user behavior, and reliability. Your objective is to build a predictive model that classifies devices as either efficient or inefficient based on these features. Accurate classification will help improve smart home designs, enhance energy efficiency, and guide better user experience strategies.*\n",
    "\n",
    "<img src=\"images/smart_home_device.png\" style=\"width:500px\"/>\n",
    "\n",
    "[Source](https://www.iotevolutionworld.com/smart-home/articles/438532-how-secure-smart-home-devices-5-steps.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by preparing our data, using the following [dataset](https://www.kaggle.com/datasets/rabieelkharoua/predict-smart-home-device-efficiency-dataset) taken from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pandas, a data table is called a DataFrame (abbreviated to df)\n",
    "data = pd.read_csv('data/smart_home_device_usage_data.csv')\n",
    "\n",
    "print(f\"There are {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "# Show the first 5 rows of the data\n",
    "data.head(5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop UserID as it is useless for predictions (you don't need to be able to perform dataframe operations like this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data.drop('UserID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into train/test set using sklearn's built-in `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('SmartHomeEfficiency', axis=1)\n",
    "y = data['SmartHomeEfficiency']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to better understand what the preprocessed data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shapes\n",
    "print('Training set shape:')\n",
    "print(f'X: {X_train.shape}, y: {y_train.shape}')\n",
    "\n",
    "print('\\nTest set shape:')\n",
    "print(f'X: {X_test.shape}, y: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to dive into our classification task of the day.\n",
    "As a reminder, the Naive Bayes Classifier revolves around Bayes' Theorem (illustrated below). Each data point is classified into the class with highest posterior probability according to Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/formula.png\" style=\"width:500px\"/>\n",
    "\n",
    "[Source](https://uc-r.github.io/naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Calculate Priors\n",
    "- Compute the prior probability for each class, which is the proportion of each class in the training dataset.\n",
    "- **Formula:**\n",
    "\n",
    " $$\n",
    "   P(C_k) = \\frac{N_k}{N} \n",
    " $$\n",
    "\n",
    "  where:\n",
    "  - $ P(C_k) $ is the prior probability of class $ C_k $\n",
    "  - $ N_k $ is the number of instances of class $ C_k $\n",
    "  - $ N $ is the total number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Expected output: array containing the prior for each class\n",
    "# Hint: Use np.bincount()\n",
    "class_counts = np.bincount(y_train)\n",
    "class_priors = class_counts / len(y_train)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Calculate Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = ['DeviceType', 'UserPreferences']\n",
    "continuous_features = ['UsageHoursPerDay', 'EnergyConsumption', 'MalfunctionIncidents', 'DeviceAgeMonths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discrete Features:**\n",
    "  - Calculate the likelihood for each value of the discrete features given each class.\n",
    "  - **Formula:**\n",
    "\n",
    "    $$\n",
    "    P(X_i = x_i | C_k) = \\frac{\\text{Count}(X_i = x_i \\land C_k)}{\\text{Count}(C_k)}\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $ P(X_i = x_i | C_k) $ is the likelihood of feature $ X_i $ taking value $ x_i $ given class $ C_k $\n",
    "    - $\\text{Count}(X_i = x_i \\land C_k)$ is the count of instances where $ X_i = x_i $ and class is $ C_k $\n",
    "    - $\\text{Count}(C_k)$ is the count of instances of class $ C_k $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discrete_likelihoods(X_train, y_train, feature, class_value):\n",
    "    \"\"\"\n",
    "    Computes the likelihoods P(feature_value | class_value) for each unique value \n",
    "    of a discrete feature given a class in a Naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data with features.\n",
    "        y_train (pd.Series): Target labels.\n",
    "        feature (str): Feature name to calculate likelihoods for.\n",
    "        class_value (any): Class value to condition on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Likelihoods of each feature value given the class.\n",
    "    \"\"\"\n",
    "    likelihoods = {}\n",
    "    values = X_train[feature].unique()\n",
    "    for value in values:\n",
    "        ### START CODE HERE ###\n",
    "        likelihoods[value] = ((X_train[feature] == value) & (y_train == class_value)).sum() / (y_train == class_value).sum()\n",
    "        ### END CODE HERE ###\n",
    "    return likelihoods\n",
    "\n",
    "\n",
    "discrete_likelihoods = {}\n",
    "for feature in discrete_features:\n",
    "    discrete_likelihoods[feature] = {}\n",
    "    for class_value in [0, 1]:\n",
    "        discrete_likelihoods[feature][class_value] = calculate_discrete_likelihoods(X_train, y_train, feature, class_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Continuous Features:**\n",
    "  - Assume Gaussian distribution for continuous features and calculate mean and variance for each feature given each class.\n",
    "  - **Formula:**\n",
    "\n",
    "    $$\n",
    "    L(X_i = x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp\\left( -\\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2} \\right)\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $ \\mu_k $ and $ \\sigma_k^2 $ are the mean and variance of feature $ X_i $ given class $ C_k $\n",
    "    - $ L(X_i = x_i | C_k) $ is the likelihood of feature $ X_i $ taking value $ x_i $ given class $ C_k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Important note</strong>\n",
    "\n",
    "For a continuous $X_i$, the probability of $X_i$ being equal to any value is always 0. We highlight the fact that the likelihood is NOT a probability using the notation $ L(X_i = x_i | C_k) $, which evaluates the _probability density function_ at $x_i$ given class $C_k$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_var(X_train, y_train, feature):\n",
    "    \"\"\"\n",
    "    Calculates the mean and variance of a feature for each class in a Naive Bayes classifier.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data with features.\n",
    "        y_train (pd.Series): Target labels.\n",
    "        feature (str): Feature name to calculate mean and variance for.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mean and variance of the feature for each class (0 and 1).\n",
    "    \"\"\"\n",
    "    likelihoods = {}\n",
    "    for class_value in [0, 1]:\n",
    "        data = X_train[feature][y_train == class_value]\n",
    "        ### START CODE HERE ###\n",
    "        mean = data.mean()\n",
    "        var = data.var()\n",
    "        ### END CODE HERE ###\n",
    "        likelihoods[class_value] = {'mean': mean, 'variance': var}\n",
    "    return likelihoods\n",
    "\n",
    "continuous_mean_var = {}\n",
    "for feature in continuous_features:\n",
    "    continuous_mean_var[feature] = calculate_mean_var(X_train, y_train, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_continuous_likelihoods(x, mean, variance):\n",
    "    \"\"\"\n",
    "    Computes the likelihood of a continuous feature using the Gaussian (normal) distribution.\n",
    "\n",
    "    Args:\n",
    "        x (float): The feature value.\n",
    "        mean (float): The mean of the feature for the class.\n",
    "        variance (float): The variance of the feature for the class.\n",
    "\n",
    "    Returns:\n",
    "        float: The likelihood of the feature value given the class.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    exponent = np.exp(-(x - mean) ** 2 / (2 * variance))\n",
    "    return (1 / np.sqrt(2 * np.pi * variance)) * exponent\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Calculate Posterior Probabilities\n",
    "- For a given instance, calculate the posterior probability for each class using Bayes' theorem.\n",
    "- We have $n_d$ discrete features and $n_c$ continuous features.\n",
    "- **Formula:**\n",
    "\n",
    "$$\n",
    "  P(C_k | X) \\propto P(C_k) \\prod_{i=1}^{n_d} P(X_i | C_k) \\prod_{j=1}^{n_c} L(X_i | C_k)\n",
    " $$\n",
    "\n",
    " or equivalently (what we will implement here):\n",
    "\n",
    "$$\n",
    " \\text{log} \\ P(C_k | X) \\propto \\text{log} \\ P(C_k) + \\sum_{i=1}^{n_d} \\text{log} P(X_i | C_k) + \\sum_{j=1}^{n_c} \\text{log} L(X_i | C_k)\n",
    " $$\n",
    "\n",
    "  where:\n",
    "  - $ P(C_k | X) $ is the posterior probability of class $ C_k $ given the instance $ X $\n",
    "  - $ P(C_k) $ is the prior probability of class $ C_k $\n",
    "  - $ P(X_i | C_k) $ is the likelihood of discrete feature $ X_i $ given class $ C_k $\n",
    "  - $ L(X_i | C_k) $ is the likelihood of continuous feature $ X_i $ given class $ C_k $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_posteriors(row, class_priors, discrete_features, continuous_features, \n",
    "                           discrete_likelihoods, continuous_mean_var, calculate_continuous_likelihoods):\n",
    "    \"\"\"\n",
    "    Computes the log posteriors for each class given a data row using both discrete and continuous features.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): The data row to classify.\n",
    "        class_priors (dict): Log prior probabilities for each class.\n",
    "        discrete_features (list): List of discrete feature names.\n",
    "        continuous_features (list): List of continuous feature names.\n",
    "        discrete_likelihoods (dict): Likelihoods of discrete features given each class.\n",
    "        continuous_mean_var (dict): Mean and variance of continuous features for each class.\n",
    "        calculate_continuous_likelihoods (function): Function to compute likelihoods for continuous features.\n",
    "\n",
    "    Returns:\n",
    "        list: Log posterior probabilities for each class.\n",
    "    \"\"\"\n",
    "    posteriors = []\n",
    "    for class_value in [0, 1]:\n",
    "        ### START CODE HERE ###\n",
    "        # Add the log of the class priors to the class posteriors\n",
    "        prior = np.log(class_priors[class_value])\n",
    "        posterior = prior\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Discrete features\n",
    "        for feature in discrete_features:\n",
    "            value = row[feature]\n",
    "            if value in discrete_likelihoods[feature][class_value]:\n",
    "                ### START CODE HERE ###\n",
    "                # Add the log of the discrete likelihoods to the class posteriors\n",
    "                posterior += np.log(discrete_likelihoods[feature][class_value][value])\n",
    "                ### END CODE HERE ###\n",
    "            else:\n",
    "                posterior += np.log(1e-6)  # Smoothing for unseen values\n",
    "\n",
    "        # Continuous features\n",
    "        for feature in continuous_features:\n",
    "            value = row[feature]\n",
    "            mean = continuous_mean_var[feature][class_value]['mean']\n",
    "            variance = continuous_mean_var[feature][class_value]['variance']\n",
    "            ### START CODE HERE ###\n",
    "            # Add the log of the continuous likelihoods to the class posteriors\n",
    "            posterior += np.log(calculate_continuous_likelihoods(value, mean, variance))\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "        posteriors.append(posterior)\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Make Predictions\n",
    "- Assign the class label with the highest posterior probability to the instance.\n",
    "- **Formula:**\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = \\arg\\max_k P(C_k | X)\n",
    "  $$\n",
    "\n",
    "  or equivalently (what we will be using here for numerical stability):\n",
    "  $$\n",
    "  \\hat{y} = \\arg\\max_k ( \\log P(C_k | X) )\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ \\hat{y} $ is the predicted class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, class_priors, discrete_features, continuous_features, \n",
    "            discrete_likelihoods, continuous_mean_var, calculate_continuous_likelihoods):\n",
    "    \"\"\"\n",
    "    Predicts class labels for a test dataset using a Naive Bayes classifier with both \n",
    "    discrete and continuous features.\n",
    "\n",
    "    Args:\n",
    "        X_test (pd.DataFrame): Test dataset to classify.\n",
    "        class_priors (dict): Prior probabilities for each class.\n",
    "        discrete_features (list): List of discrete feature names.\n",
    "        continuous_features (list): List of continuous feature names.\n",
    "        discrete_likelihoods (dict): Likelihoods of discrete features given each class.\n",
    "        continuous_mean_var (dict): Mean and variance of continuous features for each class.\n",
    "        calculate_continuous_likelihoods (function): Function to compute likelihoods for continuous features.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted class labels for the test dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _, row in X_test.iterrows():\n",
    "        # Compute log posteriors for each class\n",
    "        posteriors = compute_log_posteriors(row, class_priors, discrete_features, continuous_features,\n",
    "                                            discrete_likelihoods, continuous_mean_var, \n",
    "                                            calculate_continuous_likelihoods)\n",
    "        ### START CODE HERE ###\n",
    "        # Predict the class with the highest posterior\n",
    "        predictions.append(np.argmax(posteriors))\n",
    "        ### END CODE HERE ###\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = predict(X_test, class_priors, discrete_features, continuous_features, \n",
    "                 discrete_likelihoods, continuous_mean_var, calculate_continuous_likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model using sklearn's built-in accuracy score report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good for a naive classifier right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for your participation! In the next notebooks, you will learn to implement K-Nearest-Neighbors (KNN), an ubiquitous clustering technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
