{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bce128",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb43e8",
   "metadata": {},
   "source": [
    "<hr style=\"clear:both\">\n",
    "\n",
    "This notebook is part of a series of exercises for the CIVIL-226 Introduction to Machine Learning for Engineers course at EPFL and adapted for the ME-390. Copyright (c) 2021 [VITA](https://www.epfl.ch/labs/vita/) lab at EPFL  \n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** David Mizrahi\n",
    "\n",
    "**Adapted by:** Sabri El Amrani\n",
    "<hr style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57aaec",
   "metadata": {},
   "source": [
    "In this exercise, we'll cover the basics of the [PyTorch](https://pytorch.org) package and use it to implement simple neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384aaaf",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e97b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that PyTorch and torchvision are installed correctly\n",
    "# Note: You may need to restart your kernel and re-run this cell before running the following cells\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af56ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1e80b",
   "metadata": {},
   "source": [
    "#### For Google Colab\n",
    "You can run this notebook in Google Colab using the following link:\n",
    "https://colab.research.google.com/github/SYCAMORE-Lab/ME390-2024/blob/main/Exercises/08-neural-nets/neural_nets_pytorch_Sol.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febe6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  # Clone the entire repo to access the files\n",
    "  !git clone -l -s https://github.com/SYCAMORE-Lab/ME390-2024.git cloned-repo\n",
    "  %cd cloned-repo/Exercises/08-neural-nets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87716f",
   "metadata": {},
   "source": [
    "# Exercise 1: Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd1ec1",
   "metadata": {},
   "source": [
    "PyTorch's website offers a tutorial on the basics of PyTorch, which can be found  at the following link: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "We recommend completing this tutorial before moving on to the second exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad17d8",
   "metadata": {},
   "source": [
    "# Exercise 2: Simple neural nets for classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d31a8",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "In this exercise, we'll implement classifiers with PyTorch.\n",
    "More specifically, we'll compare a simple logistic regression model to a simple neural network on the [Electrical Fault detection and classification](https://www.kaggle.com/datasets/esathyaprakash/electrical-fault-detection-and-classification) dataset.\n",
    "\n",
    "*__Background:__ You're an electrical engineer working for a leading power systems research firm, specializing in transmission line protection and fault detection. Your team has compiled a dataset that includes measurements from various electrical power system transmission lines, documenting instances of faults and normal operating conditions. Your task is to develop a predictive model using artificial neural networks (ANNs) to detect and classify faults in transmission lines based on these measurements. This model will assist power system operators in maintaining system stability by enabling faster and more accurate fault detection, thereby minimizing power outages and enhancing the reliability of the power grid. The efficient operation of such a model is crucial for the protection of high-capacity generating power plants and the synchronized electrical power grids, ensuring the stability and robustness of the overall power system.*\n",
    "\n",
    "<img src=\"images/electrical.jpg\" style=\"width:500px\"/>\n",
    "\n",
    "[Source](https://www.usgs.gov/media/images/high-voltage-power-lines)\n",
    "\n",
    "Here is the general pipeline used to train neural networks with PyTorch:\n",
    "\n",
    "```\n",
    "1. Load the dataset\n",
    "2. Initialize a dataloader, define data transforms\n",
    "3. Define and instantiate network architecture\n",
    "4. Choose a loss function\n",
    "5. Choose an optimizer\n",
    "6. Define the training loop & number of epochs\n",
    "7. Train the model\n",
    "8. Check validation accuracy, visualize results, adjust hyper-parameters (not done in this exercise)\n",
    "9. Repeat steps 2-9 until satisfied with validation accuracy (not done in this exercise)\n",
    "10. Check test accuracy\n",
    "```\n",
    "\n",
    "*Recall, Epoch: number of times that the learning algorithm will work through the entire training set*\n",
    "\n",
    "In this exercise, we'll follow these steps to implement our classifiers. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efed59",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Helper files\n",
    "import helpers\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6b6a",
   "metadata": {},
   "source": [
    "Here is a brief description of these imported packages:\n",
    "\n",
    "**PyTorch:**\n",
    "- `torch.nn` Contains the basic building blocks to implement neural nets (incl. different types of layers and loss functions) | [Documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- `torch.nn.functional` A functional (stateless) approach to torch.nn, often used for stateless objects (e.g. ReLU) | [Documentation](https://pytorch.org/docs/stable/nn.functional.html) | [More info](https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/2)\n",
    "- `torch.optim` A package implementing various optimization algorithms, such as SGD and Adam | [Documentation](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "**`tqdm`:** Popular package used to show progress bars | [Documentation](https://tqdm.github.io/)\n",
    "\n",
    "**`helpers`**: Contains functions to help visualize data and predictions\n",
    "\n",
    "**`metrics`:** Contains two simple classes that help keep track and compute the loss and accuracy over a training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb785059",
   "metadata": {},
   "source": [
    "## 2. Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8495e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Dataset\n",
    "\n",
    "Let's start by loading our dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data\n",
    "file_path = 'data/fault_classification.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the features and target\n",
    "X = data[['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']]\n",
    "y = data['Fault_Type']\n",
    "\n",
    "# Encode categorical labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use long tensor for classification labels\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Samples in training data: {len(train_dataset)}\")\n",
    "print(f\"Samples in test data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mapping from target value to class name using label_encoder.classes_\n",
    "class_names = label_encoder.classes_\n",
    "class_name_mapping = {class_id: class_name for class_id, class_name in enumerate(class_names)}\n",
    "print(class_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3118bc7",
   "metadata": {},
   "source": [
    "### 2.2. Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cc8b3",
   "metadata": {},
   "source": [
    "When training neural networks, we usually use mini-batches of data for each forward + backward pass. \n",
    "\n",
    "In order to obtain those mini-batches, we must pass our dataset through `torch.utils.DataLoader`, which combines the dataset and a sampler, and returns an iterable over the data and labels of our dataset.\n",
    "\n",
    "In this exercise, we'll pick a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create TensorDataset and DataLoader for test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd2ace",
   "metadata": {},
   "source": [
    "Let's take a look at one of our batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cf8e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features, target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d792",
   "metadata": {},
   "source": [
    "Let's check the size of a batch of our dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040f7cf",
   "metadata": {},
   "source": [
    "If everything looks good, let's build our first model using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e71885",
   "metadata": {},
   "source": [
    "## 3. Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b106b81",
   "metadata": {},
   "source": [
    "First, we'll build a simple classifier (akin to logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51470f",
   "metadata": {},
   "source": [
    "###  3.1 Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36e51b",
   "metadata": {},
   "source": [
    "#### Short primer on `nn.Module`\n",
    "\n",
    "In PyTorch, each neural net architecture is a subclass of `nn.Module` ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html))\n",
    "\n",
    "To quote an [official tutorial](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html?highlight=module):\n",
    "\n",
    "> All network components should inherit from `nn.Module` and override the `forward()` method. That is about it, as far as the boilerplate is concerned. Inheriting from `nn.Module` provides functionality to your component. For example, it makes it keep track of its trainable parameters, you can swap it between CPU and GPU with the `.to(device)` method, where device can be a CPU device `torch.device(\"cpu\")` or CUDA device `torch.device(\"cuda:0\")`.\n",
    "\n",
    "\n",
    "In order to implement your model, you'll therefore need to fill in two methods:\n",
    "\n",
    "**`__init__()`**: \n",
    "\n",
    "- Initialize your layers here, so that the module can keep track of these layers' parameters. \n",
    "- It is not necessary to initialize layers with no learnable parameters (e.g. `ReLU`), as you can use  the`nn.functional` API for those if you want.\n",
    "\n",
    "**`forward()`**:\n",
    "\n",
    "- Define your model architecture here (i.e. call your layers in the desired order). You can use any of the Tensor operations in the `forward` function.\n",
    "- This function defines the computation performed at every call. The backward function (where gradients are computed) is automatically defined for you using autograd. \n",
    "\n",
    "The learnable parameters of a model are returned by `model.parameters()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498baa0",
   "metadata": {},
   "source": [
    "#### One layer neural net\n",
    "\n",
    "You should implement a one-layer network as follows:\n",
    "- A fully-connected layer from the input of shape (6,) to the output layer of shape (13,) as we have 13 different classes.\n",
    "\n",
    "Note that there is no hidden layer here. \n",
    "\n",
    "Furthermore, the output layer provides a real value corresponding to each of the 6 classes. Hence, to use this network for classification, we can use a cross-entropy loss:: [`CrossEntropyLoss()` module](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Hence, we can obtain the probability of each of the 6 classes after training the network.\n",
    "\n",
    "As a result, we also added a `predict()` method that adds this softmax layer, it'll be useful later on.\n",
    "\n",
    "Notice that we use `class` from python to define the neural network. For a brief introduction to this, go back to Section 8 of the exercise 01-python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9139ea",
   "metadata": {},
   "source": [
    "**One layer neural net:**\n",
    "\n",
    "<img src=\"images/1_layer_net.png\" style=\"width:300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b319cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerNet(nn.Module):\n",
    "    \"\"\"1-Layer Fault Classifier\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "input_size = 6\n",
    "output_size = 13\n",
    "\n",
    "model = OneLayerNet(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d59923",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question:** How many trainable parameters (weights) does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02058d0",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "6 * 13 + 13 (bias) = 91 weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37257f8e",
   "metadata": {},
   "source": [
    "### 3.2. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8888ecd",
   "metadata": {},
   "source": [
    "As mentioned, we choose Cross Entropy loss as our loss function. For training, we use Stochastic Gradient Descent (SGD) with a learning rate of 0.02 as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69ed8f",
   "metadata": {},
   "source": [
    "### 3.3. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7249946",
   "metadata": {},
   "source": [
    "The training loop can be defined as follows:\n",
    "\n",
    "```\n",
    "For each batch in the dataset:\n",
    "   1. Load the batch\n",
    "   2. Zero-out the accumulated gradients (PyTorch accumulates gradients during each iteration of an epoch. this is useful for some other neural network training tasks but not for our exercise).\n",
    "   3. Run the forward pass through your model.\n",
    "   4. Compute the loss.\n",
    "   5. Run the backward pass, i.e. compute gradients of the loss w.r.t. to the weights.\n",
    "   6. Update the weights using the optimizer.\n",
    "```\n",
    "\n",
    "Take a look at the `train()` function written for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epochs, seed=None):\n",
    "    \"\"\"\n",
    "    Train the model with the given parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to train.\n",
    "    train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    loss_fn (torch.nn.Module): The loss function.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer.\n",
    "    epochs (int): Number of epochs to train for.\n",
    "    seed (int, optional): Random seed for reproducibility. Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        helpers.set_seed(seed)\n",
    "    \n",
    "    # Initialize metrics for loss and accuracy (assuming they are custom implementations)\n",
    "    loss_metric = metrics.LossMetric()  # Placeholder for your LossMetric class\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)  # Placeholder for your AccuracyMetric class\n",
    "    \n",
    "    # Lists to store loss and accuracy for each epoch\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Iterate through the data\n",
    "        for data, target in train_loader:\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            loss_metric.update(loss.item(), data.size(0))\n",
    "            acc_metric.update(output, target)\n",
    "        \n",
    "        # Compute and store metrics for the epoch\n",
    "        epoch_loss = loss_metric.compute()\n",
    "        epoch_acc = acc_metric.compute() * 100\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Print epoch summary every 20 epochs\n",
    "        if epoch % 20 == 0 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train loss: {epoch_loss:.3f} | Train acc: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Reset metrics for next epoch\n",
    "        loss_metric.reset()\n",
    "        acc_metric.reset()\n",
    "    \n",
    "    # Plot the evolution of loss and accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e7f68",
   "metadata": {},
   "source": [
    "**Question:** How many iterations are in each epoch? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09723e14",
   "metadata": {},
   "source": [
    "**Answer:** There are 6288/32 $\\approx$ 175 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75255c85",
   "metadata": {},
   "source": [
    "Now that the train function is defined, it's time to actually train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 50 epochs\n",
    "train(model, train_loader, loss_fn, optimizer, epochs=50, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450948ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4. Test accuracy\n",
    "\n",
    "You should now have a model with approximately 35% training accuracy, but this doesn't tell the whole story. In order to actually estimate how good (or rather how bad in this case) our model is, we need to check its accuracy on the test set.\n",
    "\n",
    "Look through the function `test()` which computes the test accuracy of a given model on a specific dataset.\n",
    "\n",
    "This function iterates through a dataset once (using a DataLoader) and displays the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, seed=None):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of a neural network model on the given data.\n",
    "\n",
    "    This function evaluates the performance of a neural network model by calculating\n",
    "    the accuracy on the provided dataloader. It uses a progress bar to indicate\n",
    "    the completion status during evaluation and prints the final accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the evaluation data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        helpers.set_seed(seed)\n",
    "\n",
    "    # Initialize accuracy metric\n",
    "    acc_metric = metrics.AccuracyMetric(k=1)\n",
    "\n",
    "    # Progress bar setup\n",
    "    pbar = tqdm(total=len(dataloader), leave=True)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through data\n",
    "        for data, target in dataloader:\n",
    "            # Forward pass\n",
    "            out = model(data)\n",
    "\n",
    "            # Update accuracy metric\n",
    "            acc_metric.update(out, target)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "\n",
    "    # Compute and display accuracy\n",
    "    test_acc = acc_metric.compute() * 100\n",
    "    pbar.set_postfix_str(f\"Acc: {test_acc:.2f}%\")\n",
    "    print(f\"Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b09f4",
   "metadata": {},
   "source": [
    "**Expected result:** $\\approx$ 36% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfe45e",
   "metadata": {},
   "source": [
    "__Question:__ How does the test accuracy compare with the training accuracy? Do we have overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a99c3",
   "metadata": {},
   "source": [
    "__Answer:__ Training and test accuracies are very similar. There is no overfit. It is quite the opposite in fact, the model underfits: the logistic regression model is probably too simple for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120242d2",
   "metadata": {},
   "source": [
    "__Question:__ How does our logistic model compare with a random classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61c6de",
   "metadata": {},
   "source": [
    "__Answer:__ A random classifier would only get an accuracy of ~16.7% (as there are 6 classes). 35 % is therefore reassuringly already a good improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85034cc8",
   "metadata": {},
   "source": [
    "Let's not stop there, though, and try to do better with just a few small changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fcf41",
   "metadata": {},
   "source": [
    "## 4. Simple neural net (with hidden layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca6dac",
   "metadata": {},
   "source": [
    "Time to implement (actual) neural networks. As you'll soon see, the entire pipeline is very similar, only the network architecture changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edb481",
   "metadata": {},
   "source": [
    "#### Network architecture\n",
    "\n",
    "You should implement a three-layer fully-connected neural net (2 hidden layers) composed of the following:\n",
    "- A fully-connected layer from the input of shape (6,) to the first hidden layer of shape (10, ), with ReLU as an activation.\n",
    "- A fully-connected layer from the first hidden layer of shape (10,) to the second hidden layer of shape (10, ), with ReLU as an activation.\n",
    "- A fully-connected layer from the second hidden layer of shape (10,) to the output layer of shape (13,).\n",
    "\n",
    "Fully connected neural networks (FCNNs) are sometimes also referred to as multilayer perceptrons (MLP) in ML literature.\n",
    "\n",
    "**Note:** You can use the OneLayerNet function above and simply fill in additional information for the layers below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d893ee",
   "metadata": {},
   "source": [
    "**Three-layer neural net:**\n",
    "\n",
    "<img src=\"images/3_layer_net.png\" style=\"width:300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517273b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(nn.Module):\n",
    "    \"\"\"3-Layer neural net\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layer_size=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        ### START CODE HERE ###\n",
    "        ### Hint: you need two more linear layers here of appropriate dimension\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc3 = nn.Linear(hidden_layer_size, output_size)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        ### START CODE HERE ###\n",
    "        ### Hint: you need one more relu layer and the output layer     \n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts classes by calculating the softmax\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "input_size = 6\n",
    "output_size = 13\n",
    "\n",
    "model = ThreeLayerNet(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62478c39",
   "metadata": {},
   "source": [
    "**Question:** How many trainable parameters (weights) does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d0ef0",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- First layer: (6 * 10) + 10 (bias) = 70\n",
    "- Second layer: (10 * 10) + 10 (bias) = 110\n",
    "- Third layer: (10 * 13) + 13 (bias) = 143\n",
    "- **Total: 70 + 110 + 143 = 323 weights**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87db052",
   "metadata": {},
   "source": [
    "#### Loss & optimizer\n",
    "\n",
    "Use the same loss function and optimizer as in the previous section. Namely, the Cross-Entropy loss and SGD with lr=0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16faf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb0cfe",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeeacbd",
   "metadata": {},
   "source": [
    "Use `train()` to launch the training process, use the same number of epochs and seed as in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "train(model, train_loader, loss_fn, optimizer, epochs=50, seed=42)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d240cab",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b98dd",
   "metadata": {},
   "source": [
    "Compute the test accuracy using `test()` with a seed of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "test(model, test_loader, seed=42)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0a32e",
   "metadata": {},
   "source": [
    "**Expected result:** $\\approx$ 83 % test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf0a92",
   "metadata": {},
   "source": [
    "Great job so far! Now that we know how to train a neural network for classification, let's implement a network for regression. You will see, the pipeline will hardly change in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c744411",
   "metadata": {},
   "source": [
    "# Advanced (optional): Simple neural net for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0e960",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e3bfe",
   "metadata": {},
   "source": [
    "*__Background:__ You're a materials scientist working for a leading research institute focused on the electric breakdown behavior of materials. Your team has compiled a dataset that includes measurements of various semiconductor and insulator materials, documenting their intrinsic dielectric breakdown fields alongside several experimental and calculated features. Your task is to develop a predictive model that can estimate the intrinsic dielectric breakdown field of a material based on its properties. This model will assist in the discovery of new materials with superior dielectric properties and contribute to the formulation of phenomenological laws governing electric breakdown field strength. By understanding and predicting dielectric breakdown, you aim to enhance the reliability and efficiency of electronic devices and systems that rely on these materials.*\n",
    "\n",
    "<img src=\"images/dielectric_breakdown.png\" style=\"width:500px\"/>\n",
    "\n",
    "[Source](https://www.matsusada.com/application/ps/dielectric_breakdown_testing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fcdc35",
   "metadata": {},
   "source": [
    "### 1. Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efc35c",
   "metadata": {},
   "source": [
    "As usual, let's start by loading and pre-processing the data. The data for this exercise can be found [here](https://www.kaggle.com/datasets/chaozhuang/dielectric-breakdown-prediction-dataset/data). It is taken from the paper [From Organized High-Throughput Data to Phenomenological\n",
    "Theory using Machine Learning: The Example of Dielectric\n",
    "Breakdown](https://ramprasad.mse.gatech.edu/wp-content/uploads/2018/03/138.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/dielectric.csv\")\n",
    "df = df.drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['name', 'log_breakdown_field', 'structure', 'category'], axis = 1).values\n",
    "y = df[['log_breakdown_field']].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create TensorDataset and DataLoader for test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Samples in training data: {len(train_dataset)}\")\n",
    "print(f\"Samples in test data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32360a",
   "metadata": {},
   "source": [
    "### 2. Simple model for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb0dda",
   "metadata": {},
   "source": [
    "#### Network architecture\n",
    "\n",
    "You should implement a three-layer fully-connected neural net (2 hidden layers) composed of the following:\n",
    "- A fully-connected layer from the input of shape (8,) to the first hidden layer of shape (8,), with ReLU as an activation.\n",
    "- A fully-connected layer from the first hidden layer of shape (8,) to the second hidden layer of shape (8, ), with ReLU as an activation.\n",
    "- A fully-connected layer from the second hidden layer of shape (8,) to the output layer of shape (1,).\n",
    "\n",
    "As you can notice, network-wise, the main difference between classification and regression is the shape of the output. Rather than an output size equal to the number of classes, we have one node for the (one) real value that needs to be predicted. \n",
    "\n",
    "Notice also, that the softmax is not applied to the output during prediction. Indeed, we don't need a softmax that squeezes outputs between 0 and 1 anymore, but rather an output that can take any real value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class RegressionNet(nn.Module):\n",
    "    \"\"\"3-Layer neural net\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layer_size=8):\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ###\n",
    "        ### Hint: you need three linear layers here of appropriate dimension\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc3 = nn.Linear(hidden_layer_size, output_size)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### START CODE HERE ###\n",
    "        ### Hint: you need two relu layers and the output layer \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)  # Linear output for regression\n",
    "        return out\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts excitation current for regression task\"\"\"\n",
    "        prediction = self.forward(x)\n",
    "        return prediction  # Return raw output for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8111e1",
   "metadata": {},
   "source": [
    "Take a look at the training code below now. You might notice that it is almost identical to the code used for training a classification network. The main difference is the tracked metric. Instead of plotting the accuracy of the model, we will plot the [coefficient of determination $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) to assess the quality of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epochs, seed=None):   \n",
    "    \"\"\"\n",
    "    Trains a neural network model.\n",
    "\n",
    "    This function trains a neural network model using the provided data loader, loss function, and optimizer.\n",
    "    It tracks and plots the training loss and R² metrics over the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        loss_fn (callable): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "        seed (int, optional): Random seed for reproducibility. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if seed:\n",
    "        helpers.set_seed(seed)\n",
    "    \n",
    "    # Initialize metrics for loss and R²\n",
    "    loss_metric = metrics.LossMetric()  # Placeholder for your LossMetric class\n",
    "    r2_metric = metrics.R2Metric()  # Placeholder for your R2Metric class\n",
    "    \n",
    "    # Lists to store loss and R² values\n",
    "    train_losses = []\n",
    "    train_r2 = []\n",
    "    \n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Iterate through the data\n",
    "        for data, target in train_loader:\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            loss_metric.update(loss.item(), data.size(0))\n",
    "            r2_metric.update(output, target)\n",
    "        \n",
    "        # Compute average metrics for the epoch\n",
    "        epoch_loss = loss_metric.compute()\n",
    "        epoch_r2 = r2_metric.compute()\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_r2.append(epoch_r2)\n",
    "        \n",
    "        # Print epoch summary every 10 epochs\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train loss: {epoch_loss:.3f} - R²: {epoch_r2:.3f}\")\n",
    "        \n",
    "        # Reset metrics for next epoch\n",
    "        loss_metric.reset()\n",
    "        r2_metric.reset()\n",
    "    \n",
    "    # Plot the loss and R² values\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plotting loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plotting R²\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_r2, label='Train R²')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('R²')\n",
    "    plt.title('Training R² over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720815f",
   "metadata": {},
   "source": [
    "Let's run the training now. Notice how we also change the loss used: rather than a cross-entropy loss, we use the Mean Squared Error, similar to what we did in the linear regression notebook.\n",
    "\n",
    "We also opted for [Adam](https://www.geeksforgeeks.org/adam-optimizer/), a more performant optimizer than the usual SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 1\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Initialize the model\n",
    "model = RegressionNet(input_size, output_size)\n",
    "\n",
    "# Initialize the loss function, and the optimizer with a learning rate of 0.01 and a weight_decay of 0.4\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.4) \n",
    "\n",
    "# Train the model, for 1000 epochs and with a seed of 42\n",
    "train(model, train_loader, loss_fn, optimizer, epochs=1000, seed=42)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dbd98",
   "metadata": {},
   "source": [
    "Now comes the time to test our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, seed=None):\n",
    "    \"\"\"\n",
    "    Evaluates a neural network model.\n",
    "\n",
    "    This function evaluates a neural network model using the provided dataloader.\n",
    "    It tracks and prints the Mean Absolute Error (MAE) and R² metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the evaluation data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        helpers.set_seed(seed)\n",
    "\n",
    "    # Initialize MAE and R² metrics\n",
    "    mae_metric = metrics.MAEMetric()\n",
    "    r2_metric = metrics.R2Metric()\n",
    "\n",
    "    # Progress bar set-up\n",
    "    pbar = tqdm(total=len(dataloader), leave=True)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through data\n",
    "        for data, target in dataloader:\n",
    "            # Forward pass\n",
    "            out = model(data)\n",
    "\n",
    "            # Update MAE and R² metrics\n",
    "            mae_metric.update(out, target)\n",
    "            r2_metric.update(out, target)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "\n",
    "    # End of evaluation, show MAE and R²\n",
    "    test_mae = mae_metric.compute()\n",
    "    test_r2 = r2_metric.compute()\n",
    "    pbar.set_postfix_str(f\"MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}, Test R²: {test_r2:.4f}\")\n",
    "\n",
    "    # Reset metrics\n",
    "    mae_metric.reset()\n",
    "    r2_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Test the model with a seed of 42\n",
    "test(model, test_loader, seed=42)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee245f8d",
   "metadata": {},
   "source": [
    "You should obtain a [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) of approximately 0.43, and an $R^2$ of roughly 0.68.\n",
    "\n",
    "Not bad right? It roughly means that almost 70% of the variation of the data can be explained by our model. It's probably not the best performance that can be obtained on this data, but further tuning and longer trainings would be needed for that.\n",
    "\n",
    "Congratulations on training your first neural networks! In the next session, you will learn to train a special type of network: convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06991218",
   "metadata": {},
   "source": [
    "## (Optional) Additional PyTorch resources\n",
    "- PyTorch cheat sheet: https://pytorch.org/tutorials/beginner/ptcheat.html\n",
    "- Other PyTorch tutorials: https://pytorch.org/tutorials/index.html\n",
    "- PyTorch recipes: https://pytorch.org/tutorials/recipes/recipes_index.html (bite-sized code examples on specific PyTorch features)\n",
    "- PyTorch examples: https://github.com/pytorch/examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "05f41cfe409499c20c6ade0f680b207fd3596bcf0e119a0dff1094da52c67f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
