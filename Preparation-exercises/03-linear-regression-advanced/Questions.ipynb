{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we apply a linear regression to learn the dynamics of a system. \n",
    "\n",
    "<hr style=\"clear:both\">\n",
    "\n",
    "This exercise is part of a series of exercises for the ME-390-EPFL Foundations of Artificial Intelligence course at EPFL. Copyright (c) 2022 [Sycamore](https://www.epfl.ch/labs/sycamore/) lab at EPFL.\n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file or at https://www.opensource.org/licenses/MIT\n",
    "\n",
    "**Author(s):** Tony Wood, Andreas Schlaginhaufen, Loris di Natale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is composed of 4 parts: \n",
    "- Part 1: Data loading & pre-processing\n",
    "- Part 2: Linear regression \n",
    "- Part 3: Regularization \n",
    "- Part 4: Analysis and conclusions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task description\n",
    "\n",
    "You are asked to learn the model of a controlled ground robot from measurement data. The state of the robot is represented by its pose, $\\mathbf{x}(t) := \\begin{bmatrix}p_0(t),p_1(t),\\varphi(t)\\end{bmatrix}^\\top \\in \\mathbb{R}^3$, where $p_0$ and $p_1$ are position coordinates and $\\varphi$ is the orientation of the robot. The model of the robot is given by the following ordinary differential equation, \n",
    "\n",
    "$$ \\dot{\\mathbf{x}}(t) = \\begin{bmatrix} \\dot{p}_0(t) \\\\ \\dot{p}_1(t) \\\\ \\dot{\\varphi}(t) \\end{bmatrix} = f(\\mathbf{x}(t)) = \\begin{bmatrix} \n",
    "f_{p_0}(\\mathbf{x}(t)) \\\\\n",
    "f_{p_1}(\\mathbf{x}(t)) \\\\\n",
    "f_{\\varphi}(\\mathbf{x}(t)) \n",
    "\\end{bmatrix}, $$ \n",
    "\n",
    "for some unknown $f:\\mathbb{R}^3\\to\\mathbb{R}^3$ that you want to identify. You can assume that each component of $f$ can be represented by a linear feature model $f_i(\\mathbf{x}; {\\mathbf{w}}_i)=\\mathbf{w}_i^\\top\\mathbf{\\Phi}(\\mathbf{x})$ for $i=1,2,3$, where $\\mathbf{w}_i^\\top$ denotes the transpose of the vector $\\mathbf{w}_i$. Moreover, $\\mathbf{\\Phi}(\\mathbf{x})=\\begin{bmatrix}1, p_0,p_1,\\varphi, \\cos(\\varphi), \\sin(\\varphi)\\end{bmatrix}^\\top\\in\\mathbb{R}^{6}$ is the feature vector and $\\mathbf{w}_i\\in\\mathbb{R}^{6}$ for $i=1,2,3$ are the parameters for each component.  \n",
    "\n",
    "Your task is to use linear regression to learn the optimal model parameters $\\mathbf{w}^*_i$ for $i=1,2,3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 0: Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0.1 Imports and helpers for plotting\n",
    "Before starting the exercise, we need to generate the data to train our models\n",
    "\n",
    "Run this code to import packages and customize the settings of matplotlib to create beautiful plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "#this next line is only needed in iPython notebooks\n",
    "%matplotlib inline \n",
    "import math\n",
    "import matplotlib.font_manager as fm\n",
    "font = fm.FontProperties(size = 12)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Import helper functions\n",
    "\n",
    "To help you throughout this exercise, we prepared a few helper functions. You can find them in `functions.py`.  \n",
    "Feel free to browse this file - you shouln't need to edit anything.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import _fix_seed, generate_trajectories, plot_trajectories, generate_X_Y, separate_trajectories, plot_data, predict_and_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the state of the robot as $\\mathbf{x} := \\begin{bmatrix}p_0,p_1,\\varphi\\end{bmatrix}^\\top$, where $p_0$ and $p_1$ are position coordinates and $\\varphi$ is the orientation of the robot.\n",
    "\n",
    "The true behaviour of the differential wheeled robot are given by the following equations, \n",
    "$$ \\dot{\\mathbf{x}} = \\begin{bmatrix} \\dot{p}_0 \\\\ \\dot{p}_1 \\\\ \\dot{\\varphi} \\end{bmatrix} = f(\\mathbf{x}) = \\begin{bmatrix} \n",
    "f_{p_0}(\\mathbf{x}) \\\\\n",
    "f_{p_1}(\\mathbf{x}) \\\\\n",
    "f_{\\varphi}(\\mathbf{x}) \n",
    "\\end{bmatrix} = \\begin{bmatrix} v \\cdot \\cos(\\varphi) \\\\ v \\cdot \\sin(\\varphi) \\\\ \\omega \\end{bmatrix}$$\n",
    "where the speed, $ v$, and the turn rate, $\\omega$, are kept constant. The training data will consist of noisy state measurements sampled along different trajectories of the system.\n",
    "\n",
    "Note: A trajectory of the robot refers to the sequence of states $p_0(t), p_1(t), \\varphi(t)$ generated from a given initial condition $p_0(0), p_1(0), \\varphi(0)$ over a time interval $t \\in [0, T]$. \n",
    "\n",
    "The following simulation data is generated with parameter values, \n",
    "$$ v = 0.5 \\textrm{m/s}, $$\n",
    "$$ \\omega = - \\pi / 3.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What would the trajectories look like in the 2 dimensional space, $p_0$, $p_1$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simulating the System: train and test data\n",
    "\n",
    "Using the equations above and the helper functions `generate_trajectories` and `plot_trajectories`, you can generate a train and a test dataset as follows.  \n",
    "\n",
    "Note that we use `_fix_seed` to fix a random seed. This ensures reproducibility, i.e. all random operations can be repeated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "We generate trajectories of the robot from different initial conditions. The state variables are measured with some noise. Hence, we have noisy measurements of $p_0(t), p_1(t), \\varphi(t)$ starting from a random initial condition $p_0(0), p_1(0), \\varphi(0)$, over a time interval $t \\in [0,T]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "_fix_seed(7) # DO NOT CHANGE FOR SUBMISSIONS _fix_seed(7)\n",
    "#####\n",
    "\n",
    "number_trajectories_train = 10\n",
    "noise = 0.001\n",
    "\n",
    "\n",
    "# Generate the train data \n",
    "# This returns a set of trajectories (it is in the pandas dataframe format and can be converted to numpy)\n",
    "dfs_train = generate_trajectories(number_trajectories_train, noise)\n",
    "\n",
    "print(f'\\nNumber of dataframes:\\t{len(dfs_train)}')\n",
    "print(f'Shape of one dataframe:\\t{dfs_train[0].shape}\\n')\n",
    "\n",
    "display(dfs_train[1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data\n",
    "\n",
    "We generate the trajectories of the robot the same way as we generated the training set but from a different initial condition and different noise sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "_fix_seed(13434252) #DO NOT CHANGE FOR SUBMISSIONS _fix_seed(13434252)\n",
    "#####\n",
    "\n",
    "number_trajectories_test = 10\n",
    "noise = 0.001\n",
    "\n",
    "# Generate test data\n",
    "dfs_test = generate_trajectories(number_trajectories_test, noise)\n",
    "\n",
    "# Plot the last train and test trajectory - just one trajectory for each\n",
    "plot_trajectories([dfs_train[-1], dfs_test[-1]], labels=['Train', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate the targets\n",
    "\n",
    "As you can see, each trajectory contains information about the time step and state, i.e. $\\lbrace(t^i, \\mathbf{x}^i)\\rbrace_{i=1}^N$ with $ \\mathbf{x}^i:= \\mathbf{x}(t^i)$ sampled at an equal rate $\\Delta t=t_{i+1} - t_{i}$.  \n",
    "\n",
    "However, recall that we want to approximate $f$, i.e. the derivative $\\dot{x}$, which we don't have in the data. Hence, you first need to generate the targets $\\mathbf{y}^i := f(\\mathbf{x}^i) = \\dot{\\mathbf{x}}(t^i) $.  \n",
    "Since the true $\\dot{\\mathbf{x}}(t^i)$ is unknown, you can approximate it via the finite difference:\n",
    "$$\\dot{\\mathbf{x}}(t^i)\\approx \\dfrac{\\mathbf{x}(t^{i+1})-\\mathbf{x}(t^{i})}{t^{i+1}-t^i}.$$\n",
    "\n",
    "Your first exercise is to complete the function below to generate these targets (you can drop the last data point of each trajectory for simplicity, because you don't know $x^{N+1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate targets via finite differences.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing trajectory data\n",
    "        \n",
    "    Returns:\n",
    "        X (np.ndarray): State measurements, Array of shape (trajectory_length-1, 3)\n",
    "        Y (np.ndarray): Finite differences, Array of shape (trajectory_length-1, 3)\n",
    "        t (np.ndarray): Time vector, Array of shape (trajectory_length-1,)\n",
    "    \"\"\"\n",
    "    # here, we convert the dataframe to numpy format\n",
    "    X = df[['p_0', 'p_1', 'phi']].to_numpy()\n",
    "    t = df['t'].to_numpy()\n",
    "    Y = np.zeros((len(t)-1, 3))\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Hint: review numpy indexing array\n",
    "    # X = ...\n",
    "    # Y = ...\n",
    "    # t = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the provided `generate_X_Y` function to generate the X and Y that we will use later, both for the training and test data.  Note that this function merges the 10 training trajectories in a single dataframe. It does the same for the test trajectories. For visualization, later on, we separate the test trajectories into 10 sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data and have a look at the evaluation data we created\n",
    "df_train, df_test = generate_X_Y(dfs_train, dfs_test, generate_target)\n",
    "\n",
    "# We now have the time, the 3 states (X), and the derivatives (Y) together\n",
    "print('\\nShape of the test data:', df_test.shape, '\\n')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this added the needed (approximate) derivatives to the data. This allows us to define X and Y below.  \n",
    "Note that we use a helper function to separate the trajectories of the test set to provide nicer plots in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire data for training\n",
    "X_train = df_train[['p0', 'p1', 'phi']].to_numpy()\n",
    "Y_train = df_train[['dp0dt', 'dp1dt', 'dphidt']].to_numpy()\n",
    "X_test = df_test[['p0', 'p1', 'phi']].to_numpy()\n",
    "Y_test = df_test[['dp0dt', 'dp1dt', 'dphidt']].to_numpy()\n",
    "\n",
    "# For plotting, we only consider one trajectory at a time\n",
    "# This function separates the test data into separate trajectories\n",
    "t_test_trajs, X_test_trajs, Y_test_trajs = separate_trajectories(df_test, number_trajectories_test)\n",
    "\n",
    "print(f'We have {len(X_test_trajs)} test trajectories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Preview\n",
    "We can now check what X and Y look like, for example on the fifth test trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = 5\n",
    "\n",
    "plot_data(t_test_trajs[trajectory], X_test_trajs[trajectory], Y_test_trajs[trajectory])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Linear Regression: preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are required to learn the model parameters via linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pre-compute the features \n",
    "First, you need to implement the feature map:\n",
    "$$\\mathbf{X}:=\\begin{bmatrix} {\\mathbf{x}^1}^\\top \\\\ \\vdots \\\\ {\\mathbf{x}^N}^\\top \\end{bmatrix} \\mapsto \\mathbf{\\Phi}(\\mathbf{X}):=\\begin{bmatrix} \\mathbf{\\Phi}(\\mathbf{x}^1)^\\top \\\\ \\vdots \\\\ \\mathbf{\\Phi}(\\mathbf{x}^N)^\\top \\end{bmatrix}.$$\n",
    "Recall, $\\mathbf{x}^\\top$ denotes the transpose of the vector $\\mathbf{x}$, $\\mathbf{X}\\in \\mathbb{R}^{N\\times 3}$ is the input data matrix, and $\\mathbf{\\Phi}(\\mathbf{x})=\\begin{bmatrix}1,p_0,p_1,\\varphi, \\cos(\\varphi), \\sin(\\varphi)\\end{bmatrix}^\\top\\in\\mathbb{R}^{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below (element-wise numpy operations may be useful):\n",
    "def feature_map(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Generates feature map from input data matrix.\n",
    "    \n",
    "    Args: \n",
    "        X (np.ndarray): Input data matrix, shape (N,3)\n",
    "        \n",
    "    Returns:\n",
    "        Phi(X) (np.ndarray): Feature matrix, shape (N,6)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # ...\n",
    "    # feature_matrix = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "nfeatures = feature_map(X_train).shape[1]\n",
    "\n",
    "print(f'\\nOriginal shape of X: {X_train.shape}')\n",
    "print(f'Shape after the feature map has been applied: {feature_map(X_train).shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Define the linear model\n",
    "\n",
    "Now that the feature map is defined, you can use it to define your linear model $f_i(\\mathbf{x}; {\\mathbf{w}}_i)=\\mathbf{w}_i^\\top\\mathbf{\\Phi}(\\mathbf{x})$ for $i=1,2,3$.  \n",
    "Note that the model is the same for each component, only the parameter $w_i$ differs, so you only need to define one $f$ below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Expand the term: $\\mathbf{w}_1^\\top\\mathbf{\\Phi}(\\mathbf{x})$ into its 6 components. You may use $\\mathbf{w}_{1,j}$ to refer to $j$-th element of $\\mathbf{w}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and parameters (including bias)\n",
    "w_1 = np.zeros(nfeatures)\n",
    "w_2 = np.zeros(nfeatures)\n",
    "w_3 = np.zeros(nfeatures)\n",
    "\n",
    "# implement the linear model\n",
    "def f(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data matrix, shape (N,3)\n",
    "        w (np.ndarray): Weight vector, shape (6,)\n",
    "        \n",
    "    Returns:\n",
    "        f(X, w) (np.ndarray): Predicted targets, shape (N,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    # return ....\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "\n",
    "print(f(X_train, w_1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will learn one vector of weigths for each dimension, we hence separate the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split target into the three components to predict separately\n",
    "y_1_train = Y_train[:,0]\n",
    "y_2_train = Y_train[:,1]\n",
    "y_3_train = Y_train[:,2]\n",
    "\n",
    "y_1_test = Y_test[:,0]\n",
    "y_2_test = Y_test[:,1]\n",
    "y_3_test = Y_test[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mean Square Error\n",
    "You are now asked to implement a method to compute the Mean Squared Error (MSE) $J$, which is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}_i) = \\frac{1}{N} (\\mathbf{\\Phi}(\\mathbf{X}) \\mathbf{w}_i-\\mathbf{y})^{T} (\\mathbf{\\Phi}(\\mathbf{X}) \\mathbf{w}_i-\\mathbf{y}).\n",
    "\\end{align}$$\n",
    "Above $\\mathbf{\\Phi}(\\mathbf{X}) \\in \\mathbb{R}^{N \\times 6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "    \"\"\"Computes the Mean Square Error (MSE)\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Dataset of shape (N, 3)\n",
    "        y (np.ndarray): Labels of shape (N, )\n",
    "        w (np.ndarray): Weights of shape (6, )\n",
    "\n",
    "    Returns:\n",
    "        float: the MSE loss\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # ...\n",
    "    # loss = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test it on the first training dimension with the weight initialized as 0:\n",
    "print(f'Loss on dimension 1 with w1=0: {mse_loss(X_train, y_1_train, w_1): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Least squares method\n",
    "Estimate the optimal parameters $\\mathbf{w}_i^*$ via the least squares method seen in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Solves linear regression using least squares\n",
    "\n",
    "    Args:\n",
    "        X: Data of shape (N, 3)\n",
    "        y: Labels of shape (N, )\n",
    "\n",
    "    Returns:\n",
    "        Weight parameters of shape (6, )\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    # w = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your function to train w1, w2, and w3. You can then use the provided `predict_and_plot` function to check your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = least_squares(X_train, y_1_train)\n",
    "w_2 = least_squares(X_train, y_2_train)\n",
    "w_3 = least_squares(X_train, y_3_train)\n",
    "\n",
    "# print the train error for each dimension\n",
    "print(f'Train error on dimension 1: {mse_loss(X_train, y_1_train, w_1): .5f}')\n",
    "print(f'Test error dimension 1: {mse_loss(X_test, y_1_test, w_1): .5f}')\n",
    "print(f'Train error on dimension 2: {mse_loss(X_train, y_2_train, w_2): .5f}')\n",
    "print(f'Test error dimension 2: {mse_loss(X_test, y_2_test, w_2): .5f}')\n",
    "print(f'Train error on dimension 3 : {mse_loss(X_train, y_3_train, w_3): .5f}')\n",
    "print(f'Test error dimension 3: {mse_loss(X_test, y_3_test, w_3): .5f}')\n",
    "\n",
    "# Plot the prediction along the first test trajectory as an example\n",
    "print('Test prediction:')\n",
    "for i in range(1):\n",
    "    predict_and_plot(X_test_trajs[i], Y_test_trajs[i], t_test_trajs[i], w_1, w_2, w_3, feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question:** Is the model overfitting, underfitting or performing well? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Answer:** YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also plot several trajectories together, but it becomes hard to see the difference\n",
    "predict_and_plot(X_test_trajs[:5], Y_test_trajs[:5], t_test_trajs[:5], w_1, w_2, w_3, feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Ridge regression\n",
    "\n",
    "Similarly to the classical least square regression above, you are now tasked to implement the Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question:** Explain the motivation for regularization in a supervised learning problem. How would you expect the regularization influence the generalization for this specific problem? \n",
    "\n",
    "**Answer:** YOUR ANSWER HERE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ridge regularized MSE loss function\n",
    "\n",
    "**Question:** For a component $i$, write the MSE loss function with ridge regularization. Exclude $\\mathbf{w}_{i, 0}$ from the regularization!\n",
    "\n",
    "**Answer:** YOUR ANSWER HERE.\n",
    "\n",
    "Implement the ridge regression in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X: np.ndarray, y: np.ndarray, lambda_: float) -> np.ndarray:\n",
    "    \"\"\"Solves linear regression using least squares\n",
    "\n",
    "    Args:\n",
    "        X: Data of shape (N, 3)\n",
    "        y: Labels of shape (N, )\n",
    "        lambda_: regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        Weight parameters of shape (6, )\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    # ...\n",
    "    # w = ...\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train for a set of ridge coefficients $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1e-4, 1e-2, 1.0, 1e2]\n",
    "w_1_ridge = []\n",
    "w_2_ridge = []\n",
    "w_3_ridge = []\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    print('Lambda: ', lambda_)\n",
    "    \n",
    "    w_1_ridge.append(ridge_regression(X_train, y_1_train, lambda_))\n",
    "    w_2_ridge.append(ridge_regression(X_train, y_2_train, lambda_))\n",
    "    w_3_ridge.append(ridge_regression(X_train, y_3_train, lambda_))\n",
    "\n",
    "    # print the train error for each dimension\n",
    "    print(f'Train error on dimension 1: {mse_loss(X_train, y_1_train, w_1_ridge[i]): .5f}')\n",
    "    print(f'Test error dimension 1: {mse_loss(X_test, y_1_test, w_1_ridge[i]): .5f}')\n",
    "    print(f'Train error on dimension 2: {mse_loss(X_train, y_2_train, w_2_ridge[i]): .5f}')\n",
    "    print(f'Test error dimension 2: {mse_loss(X_test, y_2_test, w_2_ridge[i]): .5f}')\n",
    "    print(f'Train error on dimension 3 : {mse_loss(X_train, y_3_train, w_3_ridge[i]): .5f}')\n",
    "    print(f'Test error dimension 3: {mse_loss(X_test, y_3_test, w_3_ridge[i]): .5f}')\n",
    "\n",
    "    print('Prediction:')\n",
    "    # Plot prediction along first trajectory\n",
    "    for j in range(1):\n",
    "        predict_and_plot(X_test_trajs[j], Y_test_trajs[j], t_test_trajs[j], w_1_ridge[i], w_2_ridge[i], w_3_ridge[i], feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Print learned weight vectors\n",
    "\n",
    "Compare the learned weight vectors $\\mathbf{w}_i$ for various values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weigt vectors\n",
    "\n",
    "print('w_1:')\n",
    "# w_1\n",
    "print('lambda = 0.00: ', w_1)\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  w_1_ridge[i])\n",
    "\n",
    "print('\\nw_2:')\n",
    "# w_2\n",
    "print('lambda = 0.00: ', w_2)\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  w_2_ridge[i])\n",
    "\n",
    "print('\\nw_3:')\n",
    "# w_1\n",
    "print('lambda = 0.00: ', w_3)\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  w_3_ridge[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Norm of parameters\n",
    "\n",
    "Compare $||\\mathbf{w}_i||_2$ for the different choices of the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two norm of weights\n",
    "\n",
    "print('norm of w_1:')\n",
    "# w_1\n",
    "print('lambda = 0.00: ', f'{math.sqrt(w_1.T @ w_1):.5f}')\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  f'{math.sqrt(w_1_ridge[i].T @ w_1_ridge[i]):.5f}')\n",
    "\n",
    "print('\\nnorm of w_2:')\n",
    "# w_1\n",
    "print('lambda = 0.00: ', f'{math.sqrt(w_2.T @ w_2):.5f}')\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  f'{math.sqrt(w_2_ridge[i].T @ w_2_ridge[i]):.5f}')\n",
    "    \n",
    "print('\\nnorm of w_3:')\n",
    "# w_1\n",
    "print('lambda = 0.00: ', f'{math.sqrt(w_3.T @ w_3):.5f}')\n",
    "for (i, lambda_) in enumerate(lambdas):\n",
    "    print(f'lambda = {lambda_:.2e}: ',  f'{math.sqrt(w_3_ridge[i].T @ w_3_ridge[i]):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis and remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to add a few cells below to illustrate you answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the impact of $\\lambda$ in the Ridge regresion?\n",
    "\n",
    "**Answer**: YOUR ANSWER HERE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**: What is the impact of the number of trajectories in the training data? Would a single trajectory suffice to identify the system for the given feature vector? Try to justify your answer experimentally and theoretically.\n",
    "\n",
    "**Answer**: YOUR YANSWER HERE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
